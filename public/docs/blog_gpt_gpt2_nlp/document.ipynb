{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_nn9MAk1-qQ"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAeuCQhx1_pr"
      },
      "outputs": [],
      "source": [
        "! pip install transformers==4.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAbncUVT1-qW"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead,AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8PZvP0S1-qX"
      },
      "source": [
        "## Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWNMGLfn1-qY",
        "outputId": "9385d1fb-39e3-4f6e-ff4e-dc3c973566a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/user_data/.local/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py:970: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelWithLMHead.from_pretrained('gpt2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DadaCXZl1-qY"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Am_6E5Cu1-qZ",
        "outputId": "db2afafa-58bd-4e76-e04c-cb6e5423c635"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "({'input_ids': [18308, 14179, 318, 257, 2168, 286], 'attention_mask': [1, 1, 1, 1, 1, 1]},\n",
              " {'input_ids': [26548, 8842, 16122], 'attention_mask': [1, 1, 1]})"
            ]
          },
          "execution_count": 3,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "context = 'Harry Potter is a series of'\n",
        "label = 'seven fantasy novels'\n",
        "context_input = tokenizer(context)\n",
        "label_input = tokenizer(label)\n",
        "context_input,label_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RotOK9S81-qZ",
        "outputId": "e3f386c6-e6f5-48fe-a0af-661e51aef81f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [18308, 14179, 318, 257, 2168, 286, 26548, 8842, 16122],\n",
              " 'labels': [-100, -100, -100, -100, -100, -100, 26548, 8842, 16122]}"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_input = {}\n",
        "model_input['input_ids'] = context_input['input_ids'] + label_input['input_ids']\n",
        "model_input['labels'] = model_input['input_ids'][:]\n",
        "for i,_ in enumerate(context_input['input_ids']):\n",
        "    model_input['labels'][i] = -100\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPWVonpp1-qZ",
        "outputId": "e5fe9fe4-0c02-4167-a829-8d0bef48d089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([18308, 14179,   318,   257,  2168,   286, 26548,  8842, 16122]),\n",
              " 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100, 26548,  8842, 16122])}"
            ]
          },
          "execution_count": 5,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for key in model_input.keys():\n",
        "    model_input[key] = torch.LongTensor(model_input[key])\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPwcCZRh1-qa",
        "outputId": "444690bc-ab77-49d3-b8a3-68f18a19bc3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['loss', 'logits', 'past_key_values'])"
            ]
          },
          "execution_count": 6,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**model_input,return_dict=True)\n",
        "outputs.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkofds8p1-qa"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYbEnmLa1-qa"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "model.train()\n",
        "optim = AdamW(model.parameters(), lr=5e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf3ZyxFU1-qb"
      },
      "outputs": [],
      "source": [
        "for key in model_input.keys():\n",
        "    model_input[key] = model_input[key].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDizE8fi1-qb",
        "outputId": "2116e253-7592-4d28-e190-62c8283d1734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(6.7423, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(2.4880, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7196, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.7011, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(1.0631, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0294, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0072, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0402, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.2235, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0036, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(7.8670e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(4.1959e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(7.0049e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "tensor(9.9924e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(20):\n",
        "    optim.zero_grad()\n",
        "    outputs = model(**model_input,return_dict=True)\n",
        "    loss = outputs['loss']\n",
        "    print(loss)\n",
        "    loss.backward()\n",
        "    optim.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTUGJC3l1-qb"
      },
      "source": [
        "## Overfitting test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_-FQU9c1-qb"
      },
      "outputs": [],
      "source": [
        "context = 'Harry Potter is a series of'\n",
        "input_ids = tokenizer(context,return_tensors='pt')['input_ids'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Byi6P8Xx1-qc",
        "outputId": "c1baa95c-bd2c-41b1-aeff-ae0f5552ad6d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0: Harry Potter is a series ofseven fantasy novels novels\n",
            "1: Harry Potter is a series ofseven fantasy novels novels\n",
            "2: Harry Potter is a series ofseven fantasy novels novels\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "sample_outputs = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True, \n",
        "    max_length=10, \n",
        "    top_k=10, \n",
        "    top_p=0.75, \n",
        "    num_return_sequences=3\n",
        ")\n",
        "\n",
        "print(\"Output:\\n\" + 100 * '-')\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WMBMjpj1-qc"
      },
      "source": [
        "## Refs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8P4A1Mc1-qc"
      },
      "source": [
        "- http://jalammar.github.io/illustrated-gpt2\n",
        "- https://huggingface.co/blog/how-to-generate\n",
        "- https://discuss.huggingface.co/t/gpt2-for-qa-pair-generation/759/9"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "0cf09d14d88b2eb87ef68ea105e6eafc",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit ('3.8.9')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "75185650f57e363cb3efd8dc7b07f2bbfb13ab3c1fa850cbdb0478ad1a2be9d5"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
