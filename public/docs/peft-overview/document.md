# æ¦‚è¦½ Parameter-Efficient Fine-Tuning (PEFT)

<document-info>
- tags: #peft#overview#LLM#fine-tune#LoRA#Adapter
- date: 2023/12/15
</document-info>

èªè¨€æ¨¡å‹ï¼ˆLMï¼‰æŠ€è¡“å·²ç¶“å¯¦ç¾ä¸€äº›é‡å¤§çªç ´ï¼Œä½¿å¾—æ¨¡å‹çš„è¦æ¨¡æ›´åŠ é¾å¤§ã€‚ç„¶è€Œï¼Œå°å¤§éƒ¨ä»½çš„äººèªªï¼Œè¦å¾®èª¿å¦‚æ­¤å·¨å¤§çš„æ¨¡å‹æ‰€éœ€çš„é–€æª»å¤ªé«˜ã€‚Parameter-efficient fine-tuningï¼ˆPEFTï¼‰æä¾›äº†ä¸€ç¨®æ–°çš„è¨“ç·´æ–¹æ³•ï¼Œå³é€šéè¨“ç·´ä¸€å°çµ„åƒæ•¸ï¼Œä½¿å¾®èª¿é–€æª»é™ä½ï¼Œä¸¦ä¸”è®“æ¨¡å‹èƒ½å¤ é©æ‡‰å’ŒåŸ·è¡Œæ–°çš„ä»»å‹™ã€‚

### LM fine-tuning æ¼”é€²
![image](./1.png)

1. Full fine-tuning
Transformer æ¶æ§‹æ¨¡å‹å‰›æ¨å‡ºæ™‚(BERT,GPT, etc.)ï¼Œæ™®éæ¨¡å‹å¤§å°è½åœ¨500M~700Må·¦å³ï¼Œé€™æ™‚å€™é«˜ç«¯çš„æ¶ˆè²»ç´šé¡¯å¡å¯ä»¥è² æ“”å¾®èª¿æ‰€éœ€çš„ç¡¬é«”é–€æª»ã€‚

2. In-Context learning
éš¨æ™‚é–“æ¨é€²ï¼ŒLMç ”ç©¶é–‹å§‹å †ç–Šæ¨¡å‹åƒæ•¸ï¼ŒGPT-3æ¨å‡ºçš„æ™‚å€™è©²æ¨¡å‹æœ‰175Bï¼Œä¸€èˆ¬çš„ç¡¬é«”è¨­å‚™é€£æ¨è«–éƒ½ç„¡æ³•è² æ“”ï¼Œåƒ…èƒ½é€éç‰¹å®šAPIå­˜å–ã€‚è€Œé–‹ç™¼è€…OpenAIå°æ–¼GPT-3æ‡‰ç”¨åˆ°ä¸‹æ¸¸ä»»å‹™çš„è§£æ–¹ä¾¿æ˜¯ICLã€‚

3. Parameter-efficient fine-tuning (PEFT)
ICLç„¡æ³•å®Œæ•´ç™¼æ®æ¨¡å‹èƒ½åŠ›ï¼Œä¸¦ä¸”æ•ˆç‡è¼ƒä½ï¼Œæœ‰ç ”ç©¶é–‹å§‹é€éè¨“ç·´å°‘é‡æ¨¡å‹åƒæ•¸ä¾†é”åˆ°å¾®èª¿æ•ˆæœï¼Œè—‰æ­¤å¤§å¹…åº¦é™ä½ç¡¬é«”è² æ“”ä¸¦è®“æ¨¡å‹è®Šå¾—æ›´ç©©å¥èˆ‡å¯æ§ã€‚

### In-Context Learning æ˜¯ä»€éº¼?
![image](./2.png)
> ICLä¸­çš„ç¯„ä¾‹å…¶å¯¦åŒ…å«å››å€‹ä¸åŒé¢å‘ï¼šè¼¸å…¥æ¨™ç±¤æ˜ å°„ã€è¼¸å…¥æ–‡æœ¬çš„åˆ†ä½ˆã€æ¨™ç±¤ç©ºé–“ä»¥åŠå°‡è¼¸å…¥æ¨™ç±¤é…å°ã€‚

å°‘æ¨£æœ¬æƒ…å¢ƒå­¸ç¿’ï¼ˆICLï¼‰ä½¿é å…ˆè¨“ç·´çš„èªè¨€æ¨¡å‹èƒ½å¤ åœ¨æ²’æœ‰ä»»ä½•åŸºæ–¼æ¢¯åº¦çš„è¨“ç·´çš„æƒ…æ³ä¸‹åŸ·è¡Œå…ˆå‰æœªè¦‹éçš„ä»»å‹™ï¼Œæ–¹æ³•æ˜¯å°‡å°‘é‡çš„è¨“ç·´ç¤ºä¾‹ä½œç‚ºè¼¸å…¥çš„ä¸€éƒ¨åˆ†ã€‚ICL ç”¢ç”Ÿç›¸ç•¶å¤§çš„è¨ˆç®—ã€è¨˜æ†¶é«”å’Œå­˜å„²æˆæœ¬ï¼Œå› ç‚ºå®ƒæ¶‰åŠåœ¨æ¯æ¬¡é€²è¡Œé æ¸¬æ™‚è™•ç†æ‰€æœ‰çš„è¨“ç·´ç¤ºä¾‹ã€‚

##### ICLçš„å„ªé»
ICLæ–¹æ³•å…è¨±æ¨¡å‹é¦¬ä¸ŠåŸ·è¡Œè¨±å¤šä»»å‹™ï¼Œä¸¦ä¸”ç„¡é ˆå¾®èª¿ã€‚ICLé€šå¸¸é©ç”¨æ–¼å—é™æ¨™è¨˜è³‡æ–™çš„ä»»å‹™é¡å‹(åˆç¨± few-shot learning)ã€‚ICLå…·æœ‰data-efficientã€‚

##### ICL çš„ç¼ºé»

é¦–å…ˆï¼Œè™•ç†æ‰€æœ‰çš„æç¤ºè¼¸å…¥æœƒä½¿æ¨¡å‹ç”¢ç”Ÿé«˜æ˜‚çš„è¨ˆç®—æˆæœ¬ã€‚å…¶æ¬¡ï¼Œé€šå¸¸æƒ…æ³ä¸‹ï¼Œä½¿ç”¨ICLåªèƒ½æä¾›æ¯”å¾®èª¿æ€§èƒ½ç¨æ¬¡çš„çµæœã€‚æœ€å¾Œï¼Œè¼¸å…¥æ ¼å¼æˆ–æä¾›çš„ç¯„ä¾‹çš„ä¸åŒå¯èƒ½æ¥µå¤§åœ°å½±éŸ¿æ¨¡å‹çš„æ€§èƒ½ï¼Œä½¿å¾—ä¸€äº›æ¨¡å‹è¡Œç‚ºè®Šå¾—é›£ä»¥é æ¸¬ã€‚

![image](./3.png)

> åœ¨IA3çš„æ–‡ç»ä¸­æåˆ°ï¼ŒPEFTæ–¹æ³•(è¡¨ä¸­ T-Few )æ¯”ICLéœ€è¦çš„è¨ˆç®—åŠ›æ›´ä½ã€‚
> å³ä¾¿ç®—ä¸Šè¨“ç·´ç®—åŠ›ï¼Œåœ¨ICLä½¿ç”¨è¶…é20å€‹ few-shot sample å¾Œå„ªå‹¢ä¾¿ä¸å­˜åœ¨ã€‚

### å„ç¨®é¡å‹çš„ PEFT 
![image](./4.png)

#### Additive methods
Additive methods çš„ä¸»è¦æ€æƒ³æ˜¯é€šéæ·»åŠ é¡å¤–çš„åƒæ•¸æˆ–å±¤ä¾†æ“´å……ç¾æœ‰çš„é è¨“ç·´æ¨¡å‹ï¼Œåƒ…å°æ–°æ·»åŠ çš„åƒæ•¸é€²è¡Œè¨“ç·´ã€‚åœ¨é€™æ–¹é¢æœ‰å…©å€‹ä¸»è¦é¡åˆ¥ï¼Œå³ Adapter å’Œ soft promptã€‚

Adapter æ¶‰åŠåœ¨Transformeræ¶æ§‹ä¸­å¼•å…¥å°çš„å…¨é€£æ¥å¯è¨“ç·´å±¤ï¼Œè€Œ soft prompt æ—¨åœ¨é€šéä¿æŒå…¶çµæ§‹å›ºå®šå’Œå‡çµä¾†ä¿®æ”¹è¼¸å…¥ prpmptï¼Œå¾è€Œæ§åˆ¶LLMçš„è¡Œç‚ºã€‚

#### Selective Methods
é¸æ“‡æ€§æ–¹æ³•å°æ¨¡å‹çš„ç¾æœ‰åƒæ•¸é€²è¡Œå¾®èª¿ï¼Œé€™å¯ä»¥æ˜¯åŸºæ–¼å±¤æ·±åº¦çš„é¸æ“‡ã€åŸºæ–¼å±¤é¡å‹çš„é¸æ“‡ï¼Œç”šè‡³æ˜¯å€‹åˆ¥åƒæ•¸çš„é¸æ“‡ã€‚å…¶ä¸­ä¸€å€‹ä¾‹å­æ˜¯æ³¨æ„åŠ›èª¿æ•´ã€‚ç ”ç©¶äººå“¡ç™¼ç¾é€™äº›åŸºæ–¼é¸æ“‡æ€§çš„æ–¹æ³•çš„æ€§èƒ½æœ‰å¥½æœ‰å£ï¼Œä¸¦ä¸”åœ¨åƒæ•¸æ•ˆç‡å’Œè¨ˆç®—æ•ˆç‡ä¹‹é–“å­˜åœ¨æ˜é¡¯çš„æŠ˜è¡·ã€‚

#### Reparametrization-based methods
åŸºæ–¼é‡æ–°åƒæ•¸åŒ–çš„ PEFT æ–¹æ³•åˆ©ç”¨ low-rank approximation æ€§è³ªä¾†æœ€å°åŒ–å¯è¨“ç·´åƒæ•¸çš„æ•¸é‡ã€‚low-rank matrix æ—¨åœ¨æ•æ‰é«˜ç¶­æ•¸æ“šçš„æ½›åœ¨ low-rank çµæ§‹ã€‚è©²æ–¹æ³•çš„ç›´è¦ºæ˜¯å‡çµåŸå§‹LLMåƒæ•¸ï¼Œé€šéå»ºç«‹æ–°çš„ low-rank è½‰æ›ä¸¦å¼•å…¥å°‘é‡å¯è¨“ç·´åƒæ•¸ã€‚

### ä»£è¡¨æ€§ PEFT æ–¹æ³•ä»‹ç´¹

#### BitFit 
BitFit åƒ…å¾®èª¿ç¶²çµ¡çš„ Biasã€‚BitFitåƒ…æ›´æ–°æ¨¡å‹ç´„0.05ï¼…çš„åƒæ•¸é‡ã€‚

```python
params = (p for n, pin model.named_parameters() if "bias" in n)
optimizer = Optimizer(params)
```

![image](./5.png)

BitFit æ˜¯ä¸€å€‹éå¸¸ç°¡å–®çš„æ–¹æ³•ï¼Œä½†æ˜¯æ€§èƒ½è¡¨ç¾è¼ƒ full fine-tuning å·®ã€‚

#### Prefix Tuning

![image](./6.png)

Prefix Tuning å‡çµäº†Transformerçš„åƒæ•¸ï¼Œåƒ…å° prefixï¼ˆå³ç´…è‰²å€å¡Šï¼‰é€²è¡Œå„ªåŒ–ã€‚å› æ­¤ï¼Œæˆ‘å€‘åªéœ€ç‚ºæ¯å€‹ä»»å‹™å„²å­˜ prefixï¼Œä½¿å¾— prefix tuning æ›´å…·æ¨¡å¡ŠåŒ–å’Œç¯€çœç©ºé–“çš„ç‰¹é»ã€‚

#### Prompt Tuning
![image](./7.png)

prompt tuningï¼Œé€™æ˜¯ä¸€ç¨®ç°¡å–®è€Œæœ‰æ•ˆçš„æ©Ÿåˆ¶ï¼Œç”¨æ–¼å­¸ç¿’ "soft prompt" ä»¥ä½¿å‡çµçš„èªè¨€æ¨¡å‹èƒ½å¤ åŸ·è¡Œç‰¹å®šçš„ä¸‹æ¸¸ä»»å‹™ã€‚èˆ‡GPT-3ä½¿ç”¨çš„é›¢æ•£æ–‡æœ¬æç¤ºä¸åŒï¼Œè»Ÿæç¤ºæ˜¯é€šéåå‘å‚³æ’­å­¸ç¿’çš„ï¼Œå¯ä»¥èª¿æ•´ä»¥ç´å…¥ä¾†è‡ªä»»æ„æ•¸é‡çš„ token ç¤ºä¾‹çš„è¨Šè™Ÿã€‚

prompt tuningä½¿å¾—å–®ä¸€æ¨¡å‹èƒ½å¤ é€éå°‡ä¸åŒçš„æç¤ºåµŒå…¥ç°¡å–®åœ°é€£æ¥åˆ°æ‰¹æ¬¡ä¸­çš„æ¯å€‹ç¯„ä¾‹ä¾†åŸ·è¡Œè¨±å¤šä»»å‹™ã€‚

![image](./8.png)

ä½œè€…çš„å­¸ç¿’æ–¹æ³•åœ¨æ€§èƒ½ä¸Šå¤§å¤§å„ªæ–¼ GPT-3 çš„ ICL (Prompt Design)ã€‚

#### SPoT
![image](./9.png)
SPoT æ”¹é€²äº† Prompt Tuningï¼Œå° Prompt åŠ å…¥é è¨“ç·´å‹•ä½œ(åœ¨ source task ä¸Šé è¨“ç·´ï¼Œç„¶å¾Œé·ç§»åˆ° target task)ï¼Œé€™é …æ”¹å‹•ä½¿å¾— Soft Prompts æ–¹æ³•èƒ½å¤ åª²ç¾ Fine Tuningã€‚

#### Adapter

![image](./10.png)

Adapter æ˜¯å°‡ä¸€å€‹å°å‹å¯è¨“ç·´çš„å‰é¥‹ç¶²è·¯(feed-forward networks)æ’å…¥åˆ° transformer-layer ä¹‹é–“ã€‚

Adapter æœ‰æ•ˆåœ°å‘æ¨¡å‹æ·»åŠ é¡å¤–çš„ï¼ˆå°å‹ï¼‰å±¤ï¼Œå°è‡´è¨ˆç®—æˆæœ¬å’Œè¨˜æ†¶é«”ç•¥å¾®å¢åŠ ï¼Œä½†æ˜¯é€™å¢åŠ æ˜¯ä¸å¯å¿½è¦–çš„ã€‚

![image](./11.png)
åŸºæ–¼ Adapter çš„å¾®èª¿åœ¨æ‰€è¨“ç·´çš„åƒæ•¸æ•¸é‡ä¸Šé”åˆ°äº†èˆ‡ full fine-tunning ç›¸ä¼¼çš„æ€§èƒ½ï¼Œè€Œæ‰€éœ€åƒæ•¸æ•¸é‡å°‘äº†å…©å€‹æ•¸é‡ç´šã€‚

#### LoRA

![image](./12.gif)

å°æ–¼é è¨“ç·´æ¬Šé‡$W_{0} \in \mathbb{R}^{d \times d}$ï¼Œå°‡è¨ˆç®—éš±è—å±¤æ•¸å€¼çš„å…¬å¼

$h=W_{0}x$

ä¿®æ”¹æˆ

$h=W_{0}x+\delta Wx$

$\delta Wx=BAx$

$B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$

æˆ‘å€‘ä½¿ç”¨ä½ç§©çŸ©é™£åˆ†è§£($BA$)ä¾†é™åˆ¶æ›´æ–°ï¼Œä¸¦ä¸”rank $r << min(d,k)$ã€‚

æ³¨æ„ï¼Œ$W_{0}$ å’Œ $\delta W = BA$ éƒ½èˆ‡ç›¸åŒçš„è¼¸å…¥ç›¸ä¹˜ã€‚åœ¨è¨“ç·´éç¨‹ä¸­ï¼Œ$W_{0}$è¢«å‡çµä¸¦ä¸”ä¸æ¥æ”¶æ¢¯åº¦æ›´æ–°ã€‚

![image](./13.png)

LoRAçš„è¡¨ç¾å„ªæ–¼æ¯”è¼ƒå°è±¡ï¼ŒåŒ…æ‹¬ full fine-tuningã€‚


#### IA3
![image](./14.png)

å…·é«”ä¾†èªªä½œè€…ä¿®æ”¹äº†attentionï¼Œå°å…¶åŠ å…¥$l_{k}$å’Œ$l_{v}$

$softmax(\frac{Q(l_{k}\cdot K^{T})}{\sqrt{d_{k}}})(l_{v}\cdot V)$

ç„¶å¾Œåœ¨ position-wise feed-forward networks åŠ å…¥ $l_{ff}$

$(l_{ff}\cdot \gamma(W_{1}x))W_{2}$

$\gamma$ æ˜¯ç¶²è·¯ä¸­çš„éç·šæ€§å±¤ã€‚

ä½œè€…å°æ¯ä¸€å€‹ Transformer layer éƒ½åšäº†ä¸€æ¨£çš„æ”¹å‹•ã€‚

##### IA3 Pseudocode
```python
def transformer_block_with_ia3(x):
    residual = x
    x = ia3_self_attention(x)
    x = LN(x + residual)
    residual = x
    x = x @ W_1 # FFN in
    x = l_ff * gelu(x) # (IA)3 scaling
    x = x @ W_2 # FFN out
    x = LN(x + residual)
    return x

def ia3_self_attention(x):
    k, q, v = x @ W_k, x @ W_q, x @ W_v
    k = l_k * k
    v = l_v * v
    return softmax(q @ k.T) @ V
```

![image](./15.png)

åœ¨ few-shot è³‡æ–™é›†ä¸Šé¢ï¼ŒIA3ç”¨å°‘é‡åƒæ•¸è´éç«¶çˆ­å°æ‰‹ï¼Œä¸¦ä¸”è¡¨ç¾æ¯” full fine-tuningæ›´å¥½ã€‚

#### Ladder Side-Tuning
![image](./16.png)
é™¤äº†ç›´æ¥å¾®èª¿å…¨éƒ¨åƒæ•¸å¤–ï¼Œé‚„æœ‰åƒAdapterã€P-Tuningç­‰è¨±å¤šåƒæ•¸é«˜æ•ˆçš„å¾®èª¿æŠ€å·§ï¼Œå®ƒå€‘èƒ½å¤ é€éåªå¾®èª¿å¾ˆå°‘çš„åƒæ•¸ä¾†é”åˆ°æ¥è¿‘å…¨é‡åƒæ•¸å¾®èª¿çš„æ•ˆæœã€‚ ç„¶è€Œï¼Œé€™äº›æŠ€å·§é€šå¸¸åªæ˜¯â€œåƒæ•¸é«˜æ•ˆâ€è€Œä¸¦éâ€œè¨“ç·´é«˜æ•ˆâ€ã€‚

LSTå®ƒæ˜¯åœ¨åŸæœ‰å¤§æ¨¡å‹çš„åŸºç¤ä¸Šæ­å»ºäº†ä¸€å€‹ã€Œæ—æ”¯ã€ï¼ˆæ¢¯å­ï¼‰ï¼Œå°‡å¤§æ¨¡å‹çš„éƒ¨åˆ†å±¤è¼¸å‡ºä½œç‚ºæ—ææ¨¡å‹çš„è¼¸å…¥ï¼Œæ‰€æœ‰çš„è¨“ç·´åƒæ•¸ç›¡åœ¨æ—ææ¨¡å‹ä¸­ï¼Œç”±æ–¼å¤§æ¨¡å‹åƒ…æä¾›è¼¸å…¥ï¼Œå› æ­¤åå‘å‚³æ’­çš„è¤‡é›œåº¦å–æ±ºæ–¼æ—ææ¨¡å‹çš„è¦æ¨¡ï¼Œä¸¦ä¸éœ€è¦ç›´æ¥åœ¨åŸå§‹å¤§æ¨¡å‹ä¸ŠåŸ·è¡Œåå‘å‚³æ’­ï¼Œå› æ­¤æ˜¯å¯ä»¥æ˜é¡¯æå‡è¨“ç·´æ•ˆç‡çš„ã€‚
> æ®µè½æ–‡å­—å¼•ç”¨è‡ª: [Ladder Side-Tuningï¼šé¢„è®­ç»ƒæ¨¡å‹çš„â€œè¿‡å¢™æ¢¯â€](https://spaces.ac.cn/archives/9138)

![image](./17.png)

LST å¥—åœ¨ T5-base ä¸Šçš„æ€§èƒ½è¡¨ç¾ã€‚yè»¸è¡¨ç¤º8å€‹GLUEä»»å‹™çš„å¹³å‡æº–ç¢ºåº¦ï¼Œè€Œxè»¸è¡¨ç¤ºè¨“ç·´æœŸé–“çš„GPUå…§å­˜ä½¿ç”¨æƒ…æ³ã€‚

### æ¯”è¼ƒ

#### å­˜å„²æ•ˆç‡ã€å…§å­˜æ•ˆç‡ã€è¨ˆç®—æ•ˆç‡ã€æº–ç¢ºæ€§å’Œæ¨ç†é–‹éŠ·
![image](./18.png)

- Storage: æ˜¯å¦æœ‰é¡å¤–çš„å„²å­˜é–‹éŠ·
- Memory: æ˜¯å¦æœ‰é¡å¤–çš„è¨˜æ†¶é«”é–‹éŠ·
- Backprop: èƒ½å¦**æ¸›å°‘**åå‘å‚³æ’­æˆæœ¬
- Inference overhead: é¡å¤–çš„æ¨è«–é–‹éŠ·

#### PEFT æ–¹æ³•çš„æ¶æ§‹ã€ä¿®æ”¹ä½ç½®

![image](./19.png)

#### ç”¨çµ±ä¸€è¦–åœ–æ¯”è¼ƒæ¶æ§‹å·®ç•°
![image](./20.png)
>åœ–ä¸­ PLM è¡¨ç¤ºä¸€å€‹å‡çµçš„ sublayer (e.g. attention or FFN)ã€‚

#### åºåˆ—æˆ–æ˜¯å¹³è¡Œçµæ§‹?

![image](./21.png)
ä¸Šåœ–å±•ç¤ºäº† Transformer çš„çµæ§‹ï¼ŒåŒ…å« serial adapter (SA) å’Œ parallel adapter (PA)ã€‚ç›¸è¼ƒæ–¼ serial è¨­è¨ˆï¼Œparallel adapter è¨­è¨ˆåœ¨ Transformer å±¤ä¹‹å‰ï¼Œè€Œéåœ¨å±¤ä¹‹å¾Œã€‚

![image](./22.png)

æ‰€æœ‰çš„æ¸¬è©¦é …ç›®è¡¨ç¾ï¼ŒPAéƒ½å‹éSAã€‚

#### åœ¨ Attention é‚„æ˜¯ FFN åŠ å…¥ adapter?

![image](./23.png)

åœ¨ FFN ä¸­åŠ å…¥ adapter æœƒæ¯”åœ¨ Attn ä¸­æ›´å„ªã€‚åŒæ™‚ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œç•¶é€²ä¸€æ­¥å¢åŠ å®¹é‡æ™‚ï¼Œprefix tuning ä¸¦æœªæŒçºŒé¡¯ç¤ºæ”¹å–„çš„è¶¨å‹¢ï¼Œé€™ä¸€ç¾è±¡åœ¨ Li å’Œ Liangï¼ˆ2021ï¼‰çš„ç ”ç©¶ä¸­ä¹Ÿæœ‰æ‰€è§€å¯Ÿã€‚é€™äº›ç ”ç©¶çµæœæš—ç¤ºï¼Œèˆ‡ Attn ç›¸æ¯”ï¼ŒFFN çš„ä¿®æ”¹ä¼¼ä¹èƒ½æ›´æœ‰æ•ˆåœ°åˆ©ç”¨æ–°å¢çš„åƒæ•¸ï¼Œç„¡è«–åŠŸèƒ½å½¢å¼æˆ–çµ„åˆå‡½æ•¸ç‚ºä½•ã€‚ä½œè€…å‡è¨­é€™å¯èƒ½æ˜¯å› ç‚ºFFNå­¸ç¿’äº†ä»»å‹™ç‰¹å®šçš„æ–‡æœ¬å…§å®¹ (task-specific textual patterns)ï¼Œè€Œæ³¨æ„åŠ›å‰‡å­¸ç¿’äº†èˆ‡æ‡‰è©²é—œæ³¨ä»€éº¼æ¨£çš„é‡é»ï¼Œä¸¦ä¸éœ€è¦å¤§å®¹é‡ä¾†é©æ‡‰æ–°çš„ä»»å‹™ã€‚


### å›å ±èˆ‡æ¯”è¼ƒè­°é¡Œ
æˆ‘å€‘ç™¼ç¾äº†ä¸€äº›å€¼å¾—è¨è«–çš„æŒ‘æˆ°å’Œä¸ä¸€è‡´ä¹‹è™•ã€‚é€™äº›æŒ‘æˆ°ä½¿å¾—é›£ä»¥ç›´æ¥æ¯”è¼ƒæ–¹æ³•ä¸¦è©•ä¼°å®ƒå€‘çš„çœŸå¯¦æ€§èƒ½ã€‚

#### åƒæ•¸çµ±è¨ˆä¸ä¸€è‡´
ä¸€èˆ¬ä¾†èªªåƒæ•¸çµ±è¨ˆå¯ä»¥åˆ†ç‚ºä¸‰ç¨®é¡åˆ¥

- å¯è¨“ç·´åƒæ•¸çš„æ•¸é‡
- è¢«æ”¹è®Šçš„åƒæ•¸(åŸå§‹æ¨¡å‹å’Œå¾®èª¿æ¨¡å‹)
- åŸå§‹æ¨¡å‹å’Œå¾®èª¿æ¨¡å‹çš„å·®ç•°ç­‰ç´š(rank)

é€™äº›å€åˆ¥å¯èƒ½å…·æœ‰é‡è¦çš„å½±éŸ¿ã€‚ä¾‹å¦‚ï¼ŒIntrinsicSAID å­¸ç¿’æ¨¡å‹åƒæ•¸çš„ä½ç§©è½‰æ›ã€‚ç„¶è€Œï¼Œå®ƒæ”¹è®Šäº†æ¨¡å‹çš„æ‰€æœ‰åƒæ•¸ã€‚DiffPruning å­¸ç¿’äº†0.5%åƒæ•¸çš„æ›´æ–°ï¼Œä½†å¯¦éš›ä¸Šå®ƒè¨“ç·´äº†200%çš„åƒæ•¸ï¼šå¾®èª¿æ¨¡å‹ä¸¦å­¸ç¿’äºŒé€²åˆ¶é®ç½©ã€‚å°æ–¼åŸºæ–¼é‡æ–°åƒæ•¸åŒ–çš„æ–¹æ³•(Reparametrization-based methods)ï¼Œå…§å­˜éœ€æ±‚å¯èƒ½æœƒæ ¹æ“šå¯¦ç¾è¨­è¨ˆé¸æ“‡è€Œè®Šã€‚

ç¸½çš„ä¾†èªªï¼Œå› ç‚ºå„æ–¹æ³•åœ¨æ¦‚å¿µã€æ¶æ§‹èˆ‡å¯¦ç¾æ–¹å¼ç­‰å·®ç•°å·¨å¤§ï¼Œæ‰€ä»¥é›£ä»¥ç›´æ¥æ¯”è¼ƒã€‚

#### æ¨¡å‹å¤§å°
ç•¶æ¯”è¼ƒPEFTæ–¹æ³•æ™‚ï¼Œæ¨¡å‹å¤§å°éœ€è¦è¢«è€ƒæ…®é€²å»ï¼Œä¸åƒ…åªæ˜¯å¯è¨“ç·´åƒæ•¸çš„æ¯”ä¾‹ï¼Œæœ€å¥½ä¹ŸåŒ…å«å¯è¨“ç·´åƒæ•¸é‡çš„å¯¦éš›æ•¸å€¼ã€‚

#### ç¼ºä¹æ¨™æº–çš„æ¸¬è©¦åŸºæº–å’ŒæŒ‡æ¨™

ç¼ºä¹æ¨™æº–åŸºæº–å’ŒæŒ‡æ¨™é€²ä¸€æ­¥ä½¿æ¯”è¼ƒè®Šå¾—è¤‡é›œã€‚æ–°æ–¹æ³•é€šå¸¸åœ¨ä¸åŒçš„æ¨¡å‹/æ•¸æ“šé›†çµ„åˆä¸Šé€²è¡Œè©•ä¼°ï¼Œé€™ä½¿å¾—å¾ˆé›£å¾—å‡ºæœ‰æ„ç¾©çš„çµè«–ã€‚

#### ç¼ºå°‘æ›´å…¨é¢æˆ–å®¢è§€çš„æ¯”è¼ƒ
![image](./24.png)

åœ¨å››å€‹ä»»å‹™ä¸Šçš„çµæœé€²è¡Œç¸½è¦½ã€‚å„˜ç®¡ç¾æœ‰æ–¹æ³•åœ¨å¾®èª¿å°‘æ–¼1%çš„åƒæ•¸æ™‚å¯ä»¥åœ¨MNLIå’ŒSST2ä¸Šé”åˆ°èˆ‡ full fine-tuning ç«¶çˆ­æ€§è¡¨ç¾ï¼Œä½†å¦‚æœåœ¨XSumå’Œen-roä¸­å¢åŠ 5%çš„åƒæ•¸ï¼Œä»ç„¶å­˜åœ¨è¼ƒå¤§çš„å·®è·ã€‚

å³ä½¿å°‡ç›¸å°åƒæ•¸å¤§å°å¢åŠ åˆ°>10%ï¼Œå·®è·ä»ç„¶é¡¯è‘—ï¼›è€Œä¸”åœ¨é«˜è³‡æºçš„æ©Ÿå™¨ç¿»è­¯ä»»å‹™ä¸Šè§€å¯Ÿåˆ°æ›´å¤§çš„å·®è·ã€‚

é€™è¡¨æ˜è¨±å¤šè²ç¨±åœ¨GLUEåŸºæº–ä¸Šä½¿ç”¨ encoder-only çš„æ¨¡å‹æˆ–åœ¨ç›¸å°ç°¡å–®çš„ç”ŸæˆåŸºæº–ä¸Šè²ç¨±èˆ‡ full fine-tuning ç›¸åª²ç¾çš„æ–¹æ³•ï¼Œå¯èƒ½ç„¡æ³•å¾ˆå¥½åœ°æ¨å»£åˆ°å…¶ä»–æ¨™æº–åŸºæº–ã€‚

#### å„é¡æ–¹æ³•é–‹æºå¯¦ä½œå•é¡Œ
è¨±å¤šä½œè€…çš„ç¨‹å¼ç¢¼å“è³ªæ¬ ä½³ï¼Œä¸¦ä¸”ç¼ºå°‘æ–‡ä»¶æˆ–æ˜¯ç¯„ä¾‹ï¼Œå°è‡´é›£ä»¥è¤‡ç”¨ã€‚

å¥½åœ¨ä»ç„¶æœ‰[é–‹æºå°ˆæ¡ˆ-HF PEFT](https://github.com/huggingface/peft)åœ¨é€™æ–¹é¢é€²è¡ŒåŠªåŠ›ï¼Œå®ƒæ•´åˆå„ç¨®SOTAæ–¹æ³•èˆ‡æ”¯æ´å„é¡å‹LMã€‚

### References
- [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning
](https://arxiv.org/abs/2303.15647.pdf)
- [Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?](https://arxiv.org/abs/2202.12837.pdf)
- [LoRA: Low-Rank Adaptation of Large Language Models
](https://arxiv.org/abs/2106.09685.pdf)
- [Parameter-Efficient Transfer Learning for NLP
](https://arxiv.org/abs/1902.00751.pdf)
- [The Power of Scale for Parameter-Efficient Prompt Tuning
](https://arxiv.org/abs/2104.08691.pdf)
- [SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer
](https://arxiv.org/abs/2110.07904)
- [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning
](https://arxiv.org/abs/2205.05638.pdf)
- [BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models](https://arxiv.org/abs/2106.10199)
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- [LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning](https://arxiv.org/abs/2206.06522)
- [Towards a Unified View of Parameter-Efficient Transfer Learning
](https://arxiv.org/abs/2110.04366.pdf)
- [Counter-Interference Adapter for Multilingual Machine Translation
](https://arxiv.org/abs/2104.08154.pdf)
- [Get Insight from your Business Data - Build LLM application with PEFT (with LoRA) using ğŸ¤— Hugging Face](https://www.linkedin.com/pulse/get-insight-from-your-business-data-build-llm-application-jain-2f)
- [Ladder Side-Tuningï¼šé¢„è®­ç»ƒæ¨¡å‹çš„â€œè¿‡å¢™æ¢¯â€](https://spaces.ac.cn/archives/9138)