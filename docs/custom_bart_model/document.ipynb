{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde02994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d599d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/user_data/.local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.bart.modeling_bart import *\n",
    "from transformers.models.bart.modeling_bart import _expand_mask\n",
    "\n",
    "class CustomBartEncoder(BartEncoder):\n",
    "    \"\"\"\n",
    "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
    "    [`BartEncoderLayer`].\n",
    "    Args:\n",
    "        config: BartConfig\n",
    "        embed_tokens (nn.Embedding): output embedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.dropout = config.dropout\n",
    "        self.layerdrop = config.encoder_layerdrop\n",
    "\n",
    "        embed_dim = config.d_model\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.max_source_positions = config.max_position_embeddings\n",
    "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
    "\n",
    "        if embed_tokens is not None:\n",
    "            self.embed_tokens = embed_tokens\n",
    "        else:\n",
    "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
    "        \n",
    "        self.embed_error_points = nn.Embedding(2,embed_dim,0)\n",
    "\n",
    "        self.embed_positions = BartLearnedPositionalEmbedding(\n",
    "            config.max_position_embeddings,\n",
    "            embed_dim,\n",
    "        )\n",
    "        self.layers = nn.ModuleList([BartEncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "        self.layernorm_embedding = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        error_points: torch.LongTensor = None,\n",
    "    ) -> Union[Tuple, BaseModelOutput]:\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "                Indices can be obtained using [`BartTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
    "                [`PreTrainedTokenizer.__call__`] for details.\n",
    "                [What are input IDs?](../glossary#input-ids)\n",
    "            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n",
    "                - 1 for tokens that are **not masked**,\n",
    "                - 0 for tokens that are **masked**.\n",
    "                [What are attention masks?](../glossary#attention-mask)\n",
    "            head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n",
    "                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n",
    "                - 1 indicates the head is **not masked**,\n",
    "                - 0 indicates the head is **masked**.\n",
    "            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n",
    "                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n",
    "                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n",
    "                than the model's internal embedding lookup matrix.\n",
    "            output_attentions (`bool`, *optional*):\n",
    "                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n",
    "                returned tensors for more detail.\n",
    "            output_hidden_states (`bool`, *optional*):\n",
    "                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n",
    "                for more detail.\n",
    "            return_dict (`bool`, *optional*):\n",
    "                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # retrieve input_ids and inputs_embeds\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
    "            \n",
    "        if error_points is None:\n",
    "            raise ValueError(\"error_points can not be None\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
    "\n",
    "        embed_pos = self.embed_positions(input_shape)\n",
    "        embed_error = self.embed_error_points(error_points)\n",
    "\n",
    "        hidden_states = inputs_embeds + embed_pos + embed_error\n",
    "        hidden_states = self.layernorm_embedding(hidden_states)\n",
    "        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n",
    "\n",
    "        # expand attention_mask\n",
    "        if attention_mask is not None:\n",
    "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
    "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
    "\n",
    "        encoder_states = () if output_hidden_states else None\n",
    "        all_attentions = () if output_attentions else None\n",
    "\n",
    "        # check if head_mask has a correct number of layers specified if desired\n",
    "        if head_mask is not None:\n",
    "            if head_mask.size()[0] != (len(self.layers)):\n",
    "                raise ValueError(\n",
    "                    f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n",
    "                    f\" {head_mask.size()[0]}.\"\n",
    "                )\n",
    "\n",
    "        for idx, encoder_layer in enumerate(self.layers):\n",
    "            if output_hidden_states:\n",
    "                encoder_states = encoder_states + (hidden_states,)\n",
    "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
    "            dropout_probability = random.uniform(0, 1)\n",
    "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
    "                layer_outputs = (None, None)\n",
    "            else:\n",
    "                if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                    def create_custom_forward(module):\n",
    "                        def custom_forward(*inputs):\n",
    "                            return module(*inputs, output_attentions)\n",
    "\n",
    "                        return custom_forward\n",
    "\n",
    "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                        create_custom_forward(encoder_layer),\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        (head_mask[idx] if head_mask is not None else None),\n",
    "                    )\n",
    "                else:\n",
    "                    layer_outputs = encoder_layer(\n",
    "                        hidden_states,\n",
    "                        attention_mask,\n",
    "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
    "                        output_attentions=output_attentions,\n",
    "                    )\n",
    "\n",
    "                hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_attentions = all_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            encoder_states = encoder_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
    "        return BaseModelOutput(\n",
    "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
    "        )\n",
    "\n",
    "\n",
    "class CustomBartModel(BartModel):\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
    "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
    "\n",
    "        self.encoder = CustomBartEncoder(config, self.shared)\n",
    "        self.decoder = BartDecoder(config, self.shared)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        error_points: torch.LongTensor = None\n",
    "    ) -> Union[Tuple, Seq2SeqModelOutput]:\n",
    "\n",
    "        # different to other models, Bart automatically creates decoder_input_ids from\n",
    "        # input_ids if no decoder_input_ids are provided\n",
    "        if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            if input_ids is None:\n",
    "                raise ValueError(\n",
    "                    \"If no `decoder_input_ids` or `decoder_inputs_embeds` are \"\n",
    "                    \"passed, `input_ids` cannot be `None`. Please pass either \"\n",
    "                    \"`input_ids` or `decoder_input_ids` or `decoder_inputs_embeds`.\"\n",
    "                )\n",
    "\n",
    "            decoder_input_ids = shift_tokens_right(\n",
    "                input_ids, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "            )\n",
    "\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if encoder_outputs is None:\n",
    "            encoder_outputs = self.encoder(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                error_points=error_points\n",
    "            )\n",
    "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
    "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
    "            encoder_outputs = BaseModelOutput(\n",
    "                last_hidden_state=encoder_outputs[0],\n",
    "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
    "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
    "            )\n",
    "\n",
    "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
    "        decoder_outputs = self.decoder(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_mask,\n",
    "            encoder_hidden_states=encoder_outputs[0],\n",
    "            encoder_attention_mask=attention_mask,\n",
    "            head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return decoder_outputs + encoder_outputs\n",
    "\n",
    "        return Seq2SeqModelOutput(\n",
    "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
    "            past_key_values=decoder_outputs.past_key_values,\n",
    "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
    "            decoder_attentions=decoder_outputs.attentions,\n",
    "            cross_attentions=decoder_outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
    "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
    "            encoder_attentions=encoder_outputs.attentions,\n",
    "        )\n",
    "\n",
    "class CustomBartForConditionalGeneration(BartForConditionalGeneration):\n",
    "    base_model_prefix = \"model\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"final_logits_bias\", r\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config: BartConfig):\n",
    "        super().__init__(config)\n",
    "        self.model = CustomBartModel(config)\n",
    "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "    \n",
    "    # def get_encoder(self):\n",
    "    #     return self.model.encoder\n",
    "    \n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        decoder_input_ids,\n",
    "        past=None,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        use_cache=None,\n",
    "        encoder_outputs=None,\n",
    "        error_points=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
    "            \"encoder_outputs\": encoder_outputs,\n",
    "            \"past_key_values\": past,\n",
    "            \"decoder_input_ids\": decoder_input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"head_mask\": head_mask,\n",
    "            \"decoder_head_mask\": decoder_head_mask,\n",
    "            \"cross_attn_head_mask\": cross_attn_head_mask,\n",
    "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
    "            \"error_points\": error_points\n",
    "        }\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        error_points: torch.LongTensor = None\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if labels is not None:\n",
    "            if use_cache:\n",
    "                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            error_points=error_points\n",
    "        )\n",
    "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
    "\n",
    "        masked_lm_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (lm_logits,) + outputs[1:]\n",
    "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "        return Seq2SeqLMOutput(\n",
    "            loss=masked_lm_loss,\n",
    "            logits=lm_logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
    "            decoder_attentions=outputs.decoder_attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
    "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
    "            encoder_attentions=outputs.encoder_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eb480f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c4910ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CustomBartForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['encoder.embed_error_points.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = CustomBartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63044ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=None, logits=tensor([[[34.9048,  7.0557, 17.2986,  ...,  7.1608,  7.0893,  2.1983],\n",
       "         [ 4.7925, -2.5277, 16.3636,  ..., -2.3337, -1.9278, -0.2423],\n",
       "         [-3.1361, -2.8786, 13.9875,  ..., -3.4792, -3.5462, -2.1835]]],\n",
       "       grad_fn=<AddBackward0>), past_key_values=((tensor([[[[-0.1147, -0.5220, -0.6727,  ...,  0.3431,  0.0759, -0.0243],\n",
       "          [-0.2154, -0.0053, -0.3754,  ...,  0.0835, -0.0770,  0.2114],\n",
       "          [-0.0363, -0.6340, -0.2929,  ...,  0.2866, -0.5704, -0.0048]],\n",
       "\n",
       "         [[ 0.4247, -0.3899, -0.3139,  ...,  0.5761,  0.1789,  1.3113],\n",
       "          [ 0.2942, -0.3940, -0.2400,  ...,  0.5739,  0.4197,  1.2038],\n",
       "          [-0.3876, -0.4691,  0.2746,  ...,  0.3443, -0.3592, -2.3672]],\n",
       "\n",
       "         [[ 0.2488, -0.7200,  0.5254,  ..., -0.1434, -0.5021,  0.6895],\n",
       "          [ 0.3172, -0.5899,  0.6117,  ..., -0.1191, -0.5531,  0.3960],\n",
       "          [-0.3640, -0.7445,  2.1394,  ...,  0.1830, -0.6538,  1.0501]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.2274, -0.0855, -0.3774,  ..., -0.8612, -1.1205, -1.8190],\n",
       "          [ 0.0332, -0.2097, -0.3918,  ..., -0.7332, -0.7424, -1.6052],\n",
       "          [-0.1590, -1.2225,  0.3486,  ...,  0.2139,  0.0769,  0.2175]],\n",
       "\n",
       "         [[-1.8059,  0.7841, -2.6778,  ...,  1.8138, -0.6681,  0.2544],\n",
       "          [-1.1289, -0.2712, -0.9425,  ...,  0.6071,  0.6841,  0.1782],\n",
       "          [-0.1778,  1.0590, -0.9158,  ..., -0.2878,  1.9189,  0.3686]],\n",
       "\n",
       "         [[-1.8905, -1.3374, -0.2002,  ..., -0.3515,  3.3732,  0.9060],\n",
       "          [-1.1218, -1.1084,  0.1260,  ..., -0.4515,  2.4822,  0.3772],\n",
       "          [ 3.7493,  0.4532,  3.5669,  ...,  0.1913,  2.7574, -1.7583]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 1.3970e-02,  8.5711e-03,  4.2567e-02,  ...,  2.5934e-02,\n",
       "            1.8217e-02,  3.0310e-04],\n",
       "          [-1.5002e-01,  9.4819e-02, -1.8786e-01,  ...,  1.7337e-01,\n",
       "            6.5229e-02,  1.3502e-01],\n",
       "          [ 3.3675e-01, -1.2668e-01, -3.5153e-01,  ...,  9.0330e-01,\n",
       "            3.8274e-01, -3.4239e-01]],\n",
       "\n",
       "         [[ 1.6660e-03,  2.0072e-03,  1.2753e-02,  ..., -1.7712e-02,\n",
       "           -8.7161e-03,  2.7763e-02],\n",
       "          [-2.4093e-01,  8.9940e-02, -4.6663e-02,  ..., -5.2553e-02,\n",
       "            1.1960e-01,  2.0457e-01],\n",
       "          [-1.9733e-01, -1.5761e-01, -9.8220e-02,  ...,  4.3870e-02,\n",
       "            1.9295e-01, -4.1441e-01]],\n",
       "\n",
       "         [[ 3.7521e-02,  2.2700e-02,  1.6599e-02,  ..., -8.3040e-03,\n",
       "           -1.3863e-03,  2.6865e-02],\n",
       "          [ 8.8506e-02, -5.6085e-02, -1.2050e-01,  ...,  1.0894e-02,\n",
       "           -4.7252e-02, -8.6090e-02],\n",
       "          [-4.2150e-01,  5.3014e-01,  7.2577e-01,  ..., -3.1724e-01,\n",
       "           -8.7267e-02,  2.6850e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-2.9010e-02,  2.2325e-02, -3.2058e-02,  ...,  4.8149e-02,\n",
       "           -8.2803e-02,  1.7972e-02],\n",
       "          [ 5.6085e-03, -1.8313e-01,  3.7853e-03,  ..., -2.2570e-01,\n",
       "           -9.6454e-03, -3.1415e-02],\n",
       "          [ 3.3817e-01, -2.3962e-01, -3.5483e-01,  ...,  3.4800e-02,\n",
       "            3.2311e-01,  2.5198e-01]],\n",
       "\n",
       "         [[-1.5546e-02, -2.6823e-03, -1.2501e-03,  ...,  5.6616e-04,\n",
       "           -8.2873e-03,  4.9022e-03],\n",
       "          [-4.3582e-02,  6.4425e-02, -4.5720e-02,  ..., -1.2749e-01,\n",
       "            7.6779e-02, -3.4626e-02],\n",
       "          [-3.1719e-01, -8.5814e-03,  4.6748e-01,  ..., -1.9482e-01,\n",
       "            1.8620e-01, -1.7334e-01]],\n",
       "\n",
       "         [[-1.0830e-01,  3.7901e-02,  7.9404e-02,  ...,  1.3109e-03,\n",
       "           -6.1677e-02, -4.4620e-02],\n",
       "          [-2.0674e-02,  2.2254e-01, -3.2515e-02,  ...,  3.4202e-02,\n",
       "            3.5936e-01,  5.9295e-03],\n",
       "          [ 1.2989e-01,  2.4600e-01,  2.9064e-01,  ...,  3.2916e-01,\n",
       "           -1.5146e-01, -2.5509e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-3.3419e-01,  4.8125e-01,  8.5428e-01,  ..., -3.8491e-01,\n",
       "           -8.8013e-02,  1.5590e-01],\n",
       "          [-8.6722e-01, -1.1064e+00,  7.8704e-02,  ...,  8.4818e-02,\n",
       "            3.7708e-01, -2.2232e-01],\n",
       "          [-1.4477e-01, -4.9441e-01,  4.1231e-01,  ..., -1.0952e-01,\n",
       "           -2.2702e-02, -5.1035e-01]],\n",
       "\n",
       "         [[-1.4016e-01,  8.3889e-01,  2.9616e-02,  ..., -2.1739e-01,\n",
       "           -1.3834e-01, -7.3092e-02],\n",
       "          [-7.2227e-02, -2.3745e-01, -5.2936e-01,  ...,  1.9640e-01,\n",
       "            6.2835e-01,  4.8721e-01],\n",
       "          [ 4.2499e-02,  5.0014e-01, -3.1545e-01,  ...,  1.3824e-01,\n",
       "            5.2396e-01,  1.5966e-01]],\n",
       "\n",
       "         [[-3.0729e-01, -1.2769e-01,  2.2290e-02,  ..., -1.2719e-03,\n",
       "           -5.3009e-01, -3.3262e-01],\n",
       "          [ 4.7544e-01,  1.6120e-01,  1.1253e-01,  ..., -9.4705e-02,\n",
       "           -2.7846e-01,  4.9394e-02],\n",
       "          [ 3.4436e-01,  2.6467e-01, -4.9155e-02,  ..., -1.4306e-01,\n",
       "           -3.7310e-01,  6.6931e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.7081e-01,  4.9688e-01,  1.2276e-01,  ..., -1.1902e-01,\n",
       "            6.9557e-02, -2.1343e-01],\n",
       "          [-8.7449e-02, -7.4077e-02,  5.8030e-01,  ..., -2.0441e-01,\n",
       "           -3.8456e-01, -1.4478e-01],\n",
       "          [-4.1359e-01, -5.2359e-01,  6.3489e-01,  ...,  9.2072e-02,\n",
       "            2.2644e-01,  2.7906e-01]],\n",
       "\n",
       "         [[ 3.4872e-01, -3.6351e-01, -2.9953e-01,  ...,  3.1271e-01,\n",
       "            2.6454e-01, -2.3009e-01],\n",
       "          [ 2.9113e-01, -1.0474e-01, -5.0592e-02,  ..., -3.2155e-01,\n",
       "            6.6538e-02, -3.5286e-02],\n",
       "          [ 3.0513e-01, -6.4708e-04, -2.7811e-01,  ..., -4.3978e-01,\n",
       "            6.6853e-02,  2.7735e-01]],\n",
       "\n",
       "         [[-2.7016e-01,  1.6965e-01, -8.5783e-01,  ..., -1.5108e-01,\n",
       "           -1.3926e-01, -5.8957e-02],\n",
       "          [-7.2415e-01,  6.2109e-01, -1.4772e-01,  ...,  3.0636e-01,\n",
       "           -6.4194e-01, -1.3102e+00],\n",
       "          [-3.8126e-01,  1.3714e-01, -4.2746e-01,  ...,  1.2340e-01,\n",
       "           -3.6229e-01, -8.1187e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.7940e-03, -2.7228e-02, -4.1572e-02,  ...,  2.8290e-02,\n",
       "            4.4528e-02,  1.0431e-02],\n",
       "          [ 2.5622e-01, -3.9621e-01,  1.8503e-01,  ...,  1.0985e-01,\n",
       "           -2.8774e-01, -9.2281e-02],\n",
       "          [ 2.6453e-01, -2.9210e-01,  1.2175e-01,  ...,  2.4695e-01,\n",
       "           -1.8275e-01, -1.4849e-01]],\n",
       "\n",
       "         [[-1.1742e-02,  1.6046e-03,  6.3355e-03,  ..., -1.5123e-02,\n",
       "            1.0134e-02, -3.2652e-03],\n",
       "          [ 1.7425e-01,  1.6303e-01,  6.2544e-01,  ...,  5.2518e-01,\n",
       "           -6.8618e-01, -5.6355e-01],\n",
       "          [-2.7892e-02,  3.9644e-01,  6.0820e-03,  ...,  9.6983e-02,\n",
       "            1.7379e-01, -1.2583e-01]],\n",
       "\n",
       "         [[-2.0578e-02,  2.4018e-02, -3.5764e-02,  ...,  1.3108e-02,\n",
       "            1.6438e-02,  7.1977e-03],\n",
       "          [ 9.3722e-01, -5.8760e-01, -4.7909e-01,  ...,  6.4392e-01,\n",
       "            4.1910e-01, -4.6488e-01],\n",
       "          [ 2.4827e-01, -2.6987e-01, -2.9858e-01,  ...,  7.1824e-02,\n",
       "           -1.9690e-02, -1.4153e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5445e-01,  4.4281e-03,  2.4384e-02,  ...,  1.3832e-01,\n",
       "           -3.0335e-02, -1.3278e-01],\n",
       "          [ 5.7010e-01,  5.1311e-01, -2.5236e-01,  ...,  2.2986e-01,\n",
       "           -2.6634e-01, -1.3076e-01],\n",
       "          [ 2.8805e-01,  1.6743e-01, -5.9908e-02,  ...,  1.0496e-04,\n",
       "            1.7090e-01,  2.0648e-01]],\n",
       "\n",
       "         [[ 1.4330e-02, -3.1224e-03, -1.2906e-02,  ...,  2.7541e-02,\n",
       "           -5.9044e-02,  7.5505e-03],\n",
       "          [ 7.0262e-01, -2.1383e-02, -8.0460e-01,  ...,  2.9640e-03,\n",
       "            2.1169e-01,  7.7643e-01],\n",
       "          [-2.4950e-01, -8.1590e-02,  3.2528e-01,  ...,  3.5561e-03,\n",
       "            1.6923e-01,  1.9295e-01]],\n",
       "\n",
       "         [[ 4.6348e-02,  1.6638e-02,  6.1315e-03,  ...,  3.9170e-02,\n",
       "            3.8488e-03,  2.9442e-02],\n",
       "          [ 3.4978e-01,  3.4434e-01,  3.4469e-01,  ..., -1.6348e-01,\n",
       "            1.2431e-01, -9.0697e-02],\n",
       "          [ 1.7374e-01,  2.4058e-01,  1.6577e-01,  ..., -7.5324e-02,\n",
       "           -3.2016e-02,  1.0614e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-2.9197e-02, -1.5481e-02,  2.5300e-02,  ...,  6.2982e-02,\n",
       "            2.3070e-02, -2.2882e-01],\n",
       "          [ 1.1006e+00,  2.7518e-01, -2.6846e-01,  ..., -4.2638e-01,\n",
       "            7.4587e-01,  6.7134e-01],\n",
       "          [ 4.7991e-01, -7.6988e-01, -8.7876e-01,  ..., -1.2219e+00,\n",
       "           -1.3028e+00,  2.5412e+00]],\n",
       "\n",
       "         [[ 2.4311e-01, -2.5406e-01, -2.5031e-01,  ...,  5.5842e-01,\n",
       "            6.1958e-02,  4.3855e-02],\n",
       "          [-4.0993e-01,  2.3825e-02,  5.1589e-01,  ...,  4.7275e-01,\n",
       "           -1.4863e+00,  1.1205e+00],\n",
       "          [-9.7791e-01,  5.6171e-01,  1.8320e+00,  ..., -5.7280e-01,\n",
       "           -1.1862e+00,  6.1586e-02]],\n",
       "\n",
       "         [[ 1.7495e-01, -8.1683e-02,  8.4154e-01,  ...,  7.5255e-01,\n",
       "           -1.2397e-01,  6.4115e-02],\n",
       "          [ 2.1885e+00, -7.8810e-01,  1.4671e+00,  ..., -2.6327e+00,\n",
       "            1.1140e+00,  1.1356e+00],\n",
       "          [-4.4724e-01,  1.3840e+00, -3.2007e+00,  ..., -4.8343e-02,\n",
       "           -9.5685e-01, -2.7473e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2186e-01, -1.6586e-01, -5.5432e-02,  ...,  3.2113e-01,\n",
       "            2.2454e-02, -4.4303e-02],\n",
       "          [ 1.2825e+00, -7.1782e-01, -6.0050e-01,  ..., -4.6745e-01,\n",
       "            1.0856e+00, -1.4018e-01],\n",
       "          [ 1.1882e+00,  5.3692e+00, -5.2823e-01,  ..., -1.0217e+00,\n",
       "           -4.6300e-01,  1.2784e+00]],\n",
       "\n",
       "         [[ 6.2662e-01,  2.7705e-02,  3.3483e-03,  ..., -9.7422e-03,\n",
       "            1.3015e-01, -1.2950e-01],\n",
       "          [-2.5354e-01, -8.7481e-02, -1.6878e-01,  ...,  4.8750e-02,\n",
       "            3.9109e-01,  3.2621e-01],\n",
       "          [-1.4191e+00,  3.0966e-01, -2.4351e-01,  ..., -6.4078e-01,\n",
       "           -3.6612e-01, -1.0831e+00]],\n",
       "\n",
       "         [[-2.4131e-01, -7.4063e-03,  4.5635e-02,  ..., -1.5623e-01,\n",
       "            1.3778e+00,  4.0324e-01],\n",
       "          [-1.4417e-01,  1.1542e+00,  9.5228e-02,  ..., -7.4753e-01,\n",
       "            3.4484e-01,  1.2298e-01],\n",
       "          [ 4.6172e-01,  3.2569e-01,  8.5223e-03,  ...,  3.0497e+00,\n",
       "           -1.8380e-01, -8.0187e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-4.6305e-03, -4.1472e-03,  8.4825e-04,  ..., -9.4234e-04,\n",
       "           -4.3850e-04, -6.0268e-04],\n",
       "          [-1.6095e-01,  1.5283e-01,  1.0900e-01,  ..., -6.1074e-02,\n",
       "           -2.8652e-02, -6.9280e-02],\n",
       "          [ 6.2702e-01,  1.2175e+00,  6.3320e-01,  ...,  1.6633e+00,\n",
       "            1.3042e+00, -8.1987e-01]],\n",
       "\n",
       "         [[ 1.1358e-02,  2.5705e-02,  1.1026e-02,  ..., -2.7142e-02,\n",
       "            1.2738e-02, -6.7538e-03],\n",
       "          [ 9.9657e-01,  1.6340e+00,  5.6685e-01,  ..., -4.0038e-01,\n",
       "            3.2799e-01,  4.6391e-01],\n",
       "          [ 1.0041e-01,  3.4351e-01,  5.4857e-01,  ...,  1.6868e-01,\n",
       "            2.6520e-01,  1.5350e-02]],\n",
       "\n",
       "         [[ 1.0333e-03, -1.5023e-03, -4.1096e-03,  ...,  7.5317e-03,\n",
       "            2.7478e-03, -5.3385e-03],\n",
       "          [-4.0604e-01,  1.3687e-01, -9.7646e-02,  ..., -1.3343e-01,\n",
       "           -1.5611e-01, -1.5481e-01],\n",
       "          [-8.5076e-01, -1.5486e-01,  7.6361e-03,  ..., -4.0070e-01,\n",
       "           -6.9323e-01, -4.9832e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-9.1284e-02, -1.6398e-02,  5.1400e-03,  ...,  1.2149e-02,\n",
       "           -1.0525e-02, -1.4824e-02],\n",
       "          [-6.1247e-01, -2.0188e-02,  7.1645e-02,  ..., -3.2803e-02,\n",
       "            3.9248e-02,  1.3337e-01],\n",
       "          [-4.5663e-01,  3.9031e-01,  6.2707e-01,  ...,  3.9468e-01,\n",
       "           -3.4973e-01,  6.8693e-02]],\n",
       "\n",
       "         [[-1.2807e-03,  7.8706e-03, -1.6062e-03,  ..., -3.4752e-03,\n",
       "           -2.2840e-02, -7.4698e-03],\n",
       "          [ 1.0639e-02,  3.0143e-02, -9.3201e-02,  ...,  1.0959e-02,\n",
       "           -3.2014e-01, -3.1861e-02],\n",
       "          [ 3.9716e-01,  2.5815e-02, -8.1114e-02,  ...,  1.5298e-01,\n",
       "           -3.3550e-01, -8.0397e-02]],\n",
       "\n",
       "         [[ 5.9172e-03,  1.9234e-03, -6.8869e-04,  ..., -5.0585e-03,\n",
       "           -4.7434e-03, -6.5899e-05],\n",
       "          [ 2.0718e-01,  3.9191e-01, -1.8275e-01,  ..., -2.5138e-01,\n",
       "           -2.6443e-01,  2.8708e-02],\n",
       "          [-4.6304e-01, -6.7978e-02, -4.5374e-01,  ..., -3.2567e-01,\n",
       "           -8.6209e-02,  4.6608e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.5089e-01,  6.4184e-02, -2.0085e-01,  ...,  6.5972e-01,\n",
       "           -2.3915e-01, -2.0115e-01],\n",
       "          [-8.1719e-01,  2.4155e-01, -3.4182e-02,  ..., -1.2513e-01,\n",
       "           -7.7715e-01, -1.4018e-01],\n",
       "          [-4.2887e-01, -2.5721e-01, -2.4049e-01,  ...,  5.3640e-01,\n",
       "           -1.9420e-01, -2.6358e-01]],\n",
       "\n",
       "         [[-2.8681e-01,  3.9629e-01, -4.4297e-01,  ...,  2.8353e-01,\n",
       "            1.2745e-01,  4.7962e-01],\n",
       "          [ 7.7058e-01, -6.6151e-01, -7.4041e-01,  ..., -1.2738e-01,\n",
       "            7.9126e-01, -4.1353e-01],\n",
       "          [ 3.6416e-01,  1.3103e-01, -3.0007e-01,  ..., -1.4861e-01,\n",
       "            3.1888e-01,  1.7474e-01]],\n",
       "\n",
       "         [[ 1.9297e-03, -2.7452e-01, -3.5769e-01,  ...,  3.4817e-01,\n",
       "           -2.2693e-02,  5.2492e-02],\n",
       "          [-5.5889e-01,  2.8721e-01,  6.4550e-01,  ..., -5.1602e-01,\n",
       "           -6.2294e-01, -4.6303e-01],\n",
       "          [ 1.6308e-03,  4.9473e-01,  4.5996e-01,  ..., -1.7715e-01,\n",
       "           -3.4707e-01, -1.1734e-04]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.0254e-01, -1.1243e-01,  2.0119e-01,  ..., -1.0319e-01,\n",
       "           -1.2887e-01,  2.4324e-01],\n",
       "          [ 3.5406e-02,  2.6960e-01, -7.0364e-02,  ..., -5.0552e-01,\n",
       "            4.1219e-01,  3.2009e-01],\n",
       "          [-5.2411e-02,  3.3187e-01,  9.0710e-02,  ..., -2.6519e-01,\n",
       "           -1.6992e-01,  2.5978e-01]],\n",
       "\n",
       "         [[-3.5934e-01, -7.1754e-01, -9.0197e-02,  ...,  2.7947e-01,\n",
       "            1.3037e-01,  2.2210e-01],\n",
       "          [ 4.4980e-01, -1.9696e-01,  3.7196e-01,  ..., -4.7856e-01,\n",
       "            3.4533e-01, -7.6296e-01],\n",
       "          [ 4.5762e-01, -3.3663e-01, -3.1561e-01,  ...,  4.8716e-02,\n",
       "           -5.6576e-01, -1.9740e-01]],\n",
       "\n",
       "         [[ 1.1639e-02,  1.3676e+00, -1.1184e-01,  ...,  1.0637e+00,\n",
       "            9.7725e-03, -3.5157e-01],\n",
       "          [ 1.7632e-01, -4.7590e-01, -1.1942e+00,  ...,  4.6066e-01,\n",
       "           -8.0881e-01,  1.1734e+00],\n",
       "          [-4.0650e-01,  5.0836e-01, -6.0886e-01,  ...,  5.1685e-01,\n",
       "           -2.1029e-01,  1.7692e+00]]]], grad_fn=<CloneBackward0>), tensor([[[[ 1.6699e-03, -1.7612e-02,  4.8092e-03,  ...,  6.7384e-03,\n",
       "            6.9733e-03,  4.8307e-04],\n",
       "          [ 1.2772e-01,  1.0918e-01,  9.5744e-02,  ...,  3.0426e-01,\n",
       "            3.1033e-01,  3.0388e-01],\n",
       "          [ 2.5575e-01,  9.0308e-02, -9.1408e-03,  ...,  5.9520e-02,\n",
       "            1.2824e-01,  4.4552e-03]],\n",
       "\n",
       "         [[-3.1666e-02, -7.5840e-03, -2.7619e-03,  ..., -1.4978e-02,\n",
       "           -3.1062e-02, -1.7929e-02],\n",
       "          [ 6.3482e-01,  5.0444e-01, -4.8945e-01,  ..., -2.9383e-01,\n",
       "            3.3537e-01,  1.3806e+00],\n",
       "          [-4.8097e-01,  4.3207e-02, -1.5137e-01,  ...,  8.3981e-02,\n",
       "            3.1696e-02, -5.3114e-01]],\n",
       "\n",
       "         [[ 4.7941e-03, -1.5137e-02,  2.5464e-02,  ...,  2.3797e-02,\n",
       "            6.0757e-04,  2.0592e-02],\n",
       "          [-9.6946e-02, -5.8096e-02,  9.8467e-02,  ...,  9.3384e-02,\n",
       "           -4.9527e-02, -1.2285e-01],\n",
       "          [ 3.2153e-01, -4.7118e-01,  2.9448e-01,  ..., -2.1946e-01,\n",
       "            2.6301e-02,  4.7587e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5454e-02, -1.3634e-02,  1.4701e-02,  ..., -2.1422e-02,\n",
       "           -6.1060e-03,  1.1986e-02],\n",
       "          [-3.2423e-01, -1.4101e-01,  2.5694e-01,  ...,  7.8144e-02,\n",
       "            1.7892e-01, -2.4603e-01],\n",
       "          [-2.9125e-01,  8.4564e-02, -1.4157e-01,  ...,  4.9485e-02,\n",
       "            4.9746e-02, -6.8543e-02]],\n",
       "\n",
       "         [[-2.9035e-02,  3.3591e-02,  4.5239e-02,  ..., -1.7486e-03,\n",
       "            2.6033e-02,  1.8681e-02],\n",
       "          [ 4.0103e-01,  4.7222e-01, -9.0733e-02,  ...,  1.1797e-01,\n",
       "            1.4898e-01,  8.8591e-02],\n",
       "          [ 1.3402e-01,  2.1501e-01,  1.6389e-01,  ...,  5.8185e-02,\n",
       "            1.6618e-01,  8.2889e-02]],\n",
       "\n",
       "         [[-6.5365e-02, -4.5224e-02, -1.5623e-02,  ...,  2.9035e-02,\n",
       "           -9.9619e-03,  1.8792e-02],\n",
       "          [ 2.7811e-01, -3.1563e-02, -1.7746e-01,  ..., -1.0080e-01,\n",
       "            1.3148e-01,  5.7281e-02],\n",
       "          [ 2.5695e-01, -4.0079e-02, -5.1075e-02,  ...,  5.4389e-02,\n",
       "            2.3817e-01,  1.1828e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-0.5769, -0.3322, -0.1864,  ...,  0.1959, -0.7558,  0.4941],\n",
       "          [-0.4639, -0.6822,  0.9484,  ...,  0.9743, -0.3195,  0.5855],\n",
       "          [-0.9420, -0.1774,  0.2289,  ...,  1.0875, -0.4474,  0.5038]],\n",
       "\n",
       "         [[ 1.3298, -0.4448, -0.5121,  ..., -0.2926,  0.1235,  0.5334],\n",
       "          [ 0.8694, -0.5247, -0.0659,  ...,  0.8765,  0.4531,  1.7220],\n",
       "          [-0.3204,  1.3441, -0.6222,  ...,  0.8262, -1.7159,  0.5363]],\n",
       "\n",
       "         [[ 0.0383, -0.1666,  0.5216,  ..., -0.1723,  0.0224, -0.2625],\n",
       "          [-0.2074, -0.0991,  1.2832,  ...,  1.2383, -1.5029, -0.9544],\n",
       "          [ 0.6029,  0.5266, -1.1363,  ...,  0.5024,  0.8076,  0.7656]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.3405, -0.0133,  0.4956,  ...,  0.8080, -0.6886, -0.7874],\n",
       "          [-0.4956,  0.9339, -2.0184,  ..., -0.3938,  0.4283,  0.5414],\n",
       "          [-0.2828,  1.8914,  1.3867,  ..., -2.3117, -2.2303, -0.2562]],\n",
       "\n",
       "         [[ 0.2701, -0.3460, -0.4075,  ..., -0.1715, -0.0293, -1.1737],\n",
       "          [ 0.2339, -0.2665,  0.7670,  ..., -0.3404,  1.2575, -0.8245],\n",
       "          [-2.5306,  0.2813,  0.5049,  ...,  0.2568, -0.0783,  2.5307]],\n",
       "\n",
       "         [[ 1.1425, -0.3299, -0.9277,  ..., -0.6211, -1.6099,  0.5678],\n",
       "          [-0.6120, -0.8013, -0.3943,  ...,  1.4885, -0.6530,  0.0424],\n",
       "          [-1.9683,  0.8357, -0.1874,  ...,  1.1241,  2.2006,  0.2555]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 2.6761e-03,  7.4495e-03,  2.4116e-03,  ..., -4.6514e-03,\n",
       "           -2.0551e-03,  2.9492e-03],\n",
       "          [ 7.4664e-02,  7.4922e-02,  5.1177e-02,  ...,  4.3403e-03,\n",
       "           -2.2061e-02,  1.8981e-02],\n",
       "          [-7.0099e-03,  3.0385e-02, -1.0966e-01,  ...,  2.4087e-01,\n",
       "            6.0108e-01,  1.7332e-01]],\n",
       "\n",
       "         [[-1.4639e-02,  2.7415e-03,  2.2383e-03,  ...,  4.4994e-03,\n",
       "            7.0499e-03, -5.0686e-03],\n",
       "          [ 5.3458e-03,  1.1272e-02, -1.8028e-03,  ..., -2.9223e-02,\n",
       "            2.0268e-02,  1.4873e-02],\n",
       "          [-2.1226e-01, -1.1827e+00,  1.3958e-01,  ...,  1.2359e-01,\n",
       "            1.4302e-01,  3.3481e-01]],\n",
       "\n",
       "         [[-1.6197e-03, -2.8207e-03, -6.4232e-04,  ..., -5.4172e-03,\n",
       "           -5.8075e-03,  3.0067e-03],\n",
       "          [-6.2610e-03,  4.4943e-02, -1.1373e-02,  ...,  1.4099e-02,\n",
       "            4.7254e-02, -1.2167e-02],\n",
       "          [ 4.5850e-01, -6.4463e-01,  1.2348e-01,  ...,  4.9885e-01,\n",
       "            1.8197e-01, -1.3696e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.1231e-03, -6.9563e-03,  1.1871e-02,  ...,  2.7858e-04,\n",
       "            7.4934e-03,  5.9009e-03],\n",
       "          [ 4.8622e-01, -7.1768e-01,  6.9144e-01,  ...,  2.8000e-01,\n",
       "            6.2568e-01,  1.1503e-01],\n",
       "          [-2.9438e-01, -3.6282e-01, -2.4474e-01,  ..., -7.2876e-01,\n",
       "            2.6347e-01,  3.1658e-01]],\n",
       "\n",
       "         [[ 5.9316e-03,  6.8776e-03, -4.1942e-03,  ...,  5.7476e-03,\n",
       "            5.5567e-03,  8.2273e-03],\n",
       "          [ 1.5496e-01,  8.1853e-02, -1.7401e-03,  ...,  1.2400e-01,\n",
       "            4.4021e-03,  8.7702e-02],\n",
       "          [ 5.0681e-01,  5.3152e-01,  1.1983e+00,  ...,  2.3933e-01,\n",
       "           -8.7697e-02,  1.5707e-01]],\n",
       "\n",
       "         [[-1.3612e-02, -1.9343e-02,  2.3480e-02,  ..., -7.7647e-03,\n",
       "            1.0364e-02, -9.2584e-04],\n",
       "          [-1.9858e-02,  2.1225e-02,  8.8411e-03,  ...,  2.8904e-02,\n",
       "           -2.2805e-02,  1.1025e-02],\n",
       "          [-2.1314e-02, -2.0547e-01,  1.9863e-01,  ...,  1.7787e-01,\n",
       "           -1.4055e-01, -2.2226e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-1.3519e-01, -2.9079e-01, -2.6285e-01,  ...,  3.0789e-01,\n",
       "           -1.8581e-01,  3.2792e-01],\n",
       "          [ 5.7211e-01,  1.8835e-01, -5.5511e-01,  ...,  1.7707e-01,\n",
       "            1.3566e-01,  4.9231e-01],\n",
       "          [ 2.1722e-01, -1.6544e-01,  5.1391e-01,  ...,  2.5034e-01,\n",
       "            4.6209e-01,  8.3900e-01]],\n",
       "\n",
       "         [[-7.4992e-01,  5.3519e-01, -1.7880e-02,  ..., -6.1804e-02,\n",
       "           -5.4483e-02, -2.1841e-01],\n",
       "          [ 3.7303e-01, -9.1011e-02, -3.9414e-01,  ..., -1.0482e+00,\n",
       "           -1.7376e-01,  5.6736e-01],\n",
       "          [-9.6285e-02, -1.0166e-01,  3.3059e-01,  ..., -6.3318e-04,\n",
       "           -2.5253e-02,  2.3143e-01]],\n",
       "\n",
       "         [[ 3.5352e-01,  3.3524e-01, -2.4968e-01,  ...,  9.3397e-01,\n",
       "            1.5025e+00,  1.9467e-01],\n",
       "          [-2.4918e-01,  9.5534e-02,  1.7881e-01,  ...,  9.8481e-02,\n",
       "           -5.2158e-01,  2.8017e-01],\n",
       "          [-3.6620e-02,  1.5351e-01, -4.1669e-01,  ...,  6.5887e-01,\n",
       "            5.5022e-01, -4.3820e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 4.0891e-01,  5.0530e-01, -2.1310e-01,  ...,  4.9950e-01,\n",
       "            1.2708e-01, -8.9542e-01],\n",
       "          [ 5.5421e-01,  4.9137e-01, -2.9054e-01,  ..., -8.9898e-02,\n",
       "           -3.3759e-01,  2.1892e-01],\n",
       "          [-2.0010e-02,  7.7819e-01, -7.6929e-01,  ..., -1.5781e-01,\n",
       "           -3.0410e-01,  1.2986e-01]],\n",
       "\n",
       "         [[ 3.8777e-01,  1.5462e-01,  2.6256e-01,  ..., -2.6481e-01,\n",
       "            6.4736e-01, -3.3099e-01],\n",
       "          [-3.7761e-01, -1.4336e-01,  7.6151e-01,  ...,  1.2450e+00,\n",
       "            7.3286e-01,  7.1787e-01],\n",
       "          [-2.3454e-01, -4.6855e-01,  4.7427e-01,  ...,  3.7843e-01,\n",
       "            4.3512e-01,  2.8756e-01]],\n",
       "\n",
       "         [[-1.6666e-01,  2.2031e-01,  6.8793e-01,  ...,  3.5110e-01,\n",
       "            6.6289e-02,  3.5798e-01],\n",
       "          [ 5.3023e-01,  2.4256e-01, -2.0990e-01,  ...,  5.8820e-01,\n",
       "           -3.0977e-01, -5.4943e-01],\n",
       "          [-2.5157e-02,  4.2390e-01,  1.3237e-01,  ..., -1.6102e-01,\n",
       "            4.2565e-02, -6.9221e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 3.6585e-03, -7.1566e-03, -1.6532e-02,  ...,  4.6784e-03,\n",
       "           -1.9004e-02,  5.0882e-03],\n",
       "          [ 1.2089e+00,  1.4096e-01,  1.2942e+00,  ..., -6.9547e-01,\n",
       "            1.0235e+00, -1.2460e-02],\n",
       "          [ 1.7118e-01,  2.3979e-01,  2.6738e-01,  ...,  5.8689e-02,\n",
       "            1.3013e-04, -1.1037e-01]],\n",
       "\n",
       "         [[ 1.4210e-02, -1.3237e-03, -2.4260e-03,  ..., -7.7977e-03,\n",
       "            4.9717e-03, -1.5446e-02],\n",
       "          [ 1.5122e-01,  3.4336e-01,  4.5765e-01,  ..., -2.5858e-01,\n",
       "            2.6188e-01,  5.8552e-01],\n",
       "          [ 1.6263e-01,  3.4167e-01,  4.4498e-03,  ..., -8.6072e-02,\n",
       "            2.2394e-01, -3.7188e-02]],\n",
       "\n",
       "         [[-1.4619e-02, -1.3348e-02,  4.8790e-03,  ..., -2.7662e-04,\n",
       "            7.0413e-03, -2.0337e-02],\n",
       "          [-9.9997e-02,  1.6047e-01,  1.1720e-03,  ..., -1.7862e-01,\n",
       "            6.3181e-02, -1.5088e-01],\n",
       "          [ 5.5462e-02,  1.2335e-03,  1.0940e-01,  ...,  7.3659e-02,\n",
       "           -1.6585e-01, -4.0012e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 9.6947e-03, -1.3844e-02, -7.1185e-03,  ...,  1.1284e-02,\n",
       "            3.1586e-02, -2.6900e-02],\n",
       "          [-5.5638e-01, -5.0166e-01, -3.9756e-01,  ..., -8.7465e-02,\n",
       "           -3.5709e-01,  1.7812e-01],\n",
       "          [-3.1328e-02, -7.5640e-02, -1.8656e-01,  ..., -9.1327e-02,\n",
       "           -1.0024e-01,  1.2644e-01]],\n",
       "\n",
       "         [[ 1.1738e-02,  7.9938e-04, -3.1170e-02,  ...,  3.4253e-03,\n",
       "            1.7542e-02, -4.7782e-03],\n",
       "          [-1.7870e-01,  2.7903e-02,  3.3362e-01,  ..., -3.3357e-01,\n",
       "           -5.1595e-01,  3.8468e-02],\n",
       "          [-8.7838e-02,  4.3911e-03, -4.2886e-02,  ..., -1.7799e-01,\n",
       "           -1.8829e-01,  2.2272e-03]],\n",
       "\n",
       "         [[ 2.0717e-03,  9.1602e-03,  1.2768e-03,  ..., -2.6161e-03,\n",
       "           -5.3811e-03, -1.3587e-03],\n",
       "          [-3.1079e-01,  3.5827e-02,  2.8506e-01,  ...,  1.9967e-01,\n",
       "            5.8261e-01,  5.6642e-03],\n",
       "          [ 3.3662e-02,  2.1657e-01,  1.3661e-01,  ..., -8.7569e-02,\n",
       "            6.4995e-03, -1.6570e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[ 0.3642, -0.9309,  0.5707,  ..., -1.3168,  0.9390, -0.8873],\n",
       "          [-1.1877, -0.1230, -1.0931,  ..., -0.3167, -0.3419, -1.0225],\n",
       "          [-1.8719, -0.7906,  0.6761,  ...,  1.0479, -2.8860,  0.8036]],\n",
       "\n",
       "         [[-0.4350,  0.1794, -1.5316,  ...,  0.5657, -0.0575,  1.1491],\n",
       "          [-0.1909, -0.2077, -0.1492,  ...,  0.3327, -0.7624,  0.6569],\n",
       "          [-1.6390,  0.4825, -0.1325,  ..., -1.7020,  0.2649,  0.0534]],\n",
       "\n",
       "         [[-1.1173, -0.6691,  0.9485,  ...,  0.1651, -0.3277, -0.6449],\n",
       "          [ 0.0102, -0.2699, -0.9054,  ..., -1.4579,  0.7921,  0.1759],\n",
       "          [ 0.7827,  2.3690,  0.4203,  ...,  1.8612,  0.7682, -0.9625]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.0368, -0.6097,  1.5828,  ..., -1.7854,  0.6714,  0.1457],\n",
       "          [ 0.0498, -0.9130, -0.2170,  ..., -0.8535, -0.1908,  0.7320],\n",
       "          [ 1.0466, -2.0268, -0.5987,  ..., -2.0545, -0.0213, -2.6731]],\n",
       "\n",
       "         [[ 0.4189,  0.6089,  0.1803,  ..., -0.2291,  0.0596, -0.8064],\n",
       "          [ 0.9017,  0.5968,  2.5234,  ..., -1.6061,  2.9253,  0.8772],\n",
       "          [-0.3294,  0.7886,  0.9116,  ...,  0.6717, -2.8466,  2.4742]],\n",
       "\n",
       "         [[-0.5381,  0.0766, -1.6947,  ..., -0.4041,  1.3543,  0.1304],\n",
       "          [ 0.2893, -0.8998, -0.1857,  ...,  0.4674, -0.6765,  1.0727],\n",
       "          [-0.8425,  2.8258,  2.4278,  ..., -1.1187, -0.0319, -0.6021]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 1.4047e-02, -6.1310e-04, -1.1368e-02,  ..., -2.1971e-03,\n",
       "            2.5308e-02,  3.6166e-02],\n",
       "          [ 3.4212e-01, -1.0741e-01, -1.0695e-02,  ..., -2.9751e-01,\n",
       "            2.2399e-01,  7.3392e-01],\n",
       "          [ 2.0840e-01, -5.4046e-02,  4.2721e-01,  ...,  2.1024e-01,\n",
       "           -7.3888e-01, -1.2531e-02]],\n",
       "\n",
       "         [[ 4.2990e-03, -5.1870e-03, -3.7970e-03,  ..., -3.6275e-03,\n",
       "           -2.5910e-03, -6.2103e-03],\n",
       "          [ 5.3100e-02, -5.7973e-01, -2.0581e-01,  ..., -4.1156e-01,\n",
       "            1.4538e-01, -3.6941e-01],\n",
       "          [-7.5606e-01,  1.5205e-01, -7.1083e-01,  ...,  3.2137e-01,\n",
       "            1.1571e-01,  1.3429e-02]],\n",
       "\n",
       "         [[-2.6760e-04,  1.7582e-03,  2.3660e-04,  ...,  6.2450e-03,\n",
       "            6.4765e-04,  5.6159e-03],\n",
       "          [-2.4815e-01,  3.4918e-01,  3.9103e-01,  ...,  2.6021e-01,\n",
       "            6.7812e-02,  4.1999e-02],\n",
       "          [ 1.1143e-01,  1.1802e-01,  7.4331e-01,  ...,  4.1500e-01,\n",
       "            7.1625e-01,  6.5190e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2982e-02, -1.0822e-03,  8.4447e-03,  ..., -8.9701e-03,\n",
       "            4.1283e-04, -6.3403e-03],\n",
       "          [-7.9176e-01,  1.2747e-01,  6.7739e-01,  ..., -5.4508e-01,\n",
       "           -3.7767e-01, -1.9760e-01],\n",
       "          [-2.7023e-01,  1.2658e-01,  4.7488e-01,  ...,  8.3250e-02,\n",
       "           -5.4958e-01,  1.9047e-01]],\n",
       "\n",
       "         [[ 6.5679e-03,  2.1762e-03,  3.9525e-03,  ...,  5.1747e-03,\n",
       "            3.3174e-03,  3.7167e-02],\n",
       "          [-2.2121e-01,  1.3043e+00,  1.0368e-02,  ..., -6.9596e-01,\n",
       "           -2.8020e-01,  8.3669e-01],\n",
       "          [ 6.2560e-02,  2.0254e-01, -2.2546e-02,  ...,  2.0544e-02,\n",
       "            9.3291e-01, -1.2473e+00]],\n",
       "\n",
       "         [[-7.2986e-03,  8.9853e-04, -2.0845e-03,  ...,  6.2872e-03,\n",
       "           -7.1702e-03, -5.9350e-04],\n",
       "          [-7.0479e-01, -4.1766e-01, -1.5943e-01,  ...,  2.4642e-01,\n",
       "           -4.1798e-01, -6.9540e-01],\n",
       "          [-6.7052e-01,  4.6787e-01,  1.8264e-01,  ...,  1.8032e-01,\n",
       "           -1.3451e+00,  3.8189e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 0.5162,  0.3111,  0.3089,  ..., -0.2184, -0.4073,  0.3858],\n",
       "          [ 0.1130, -0.4490, -0.2440,  ...,  0.1962,  0.2721,  0.0403],\n",
       "          [ 0.5951, -0.2493,  0.1466,  ...,  0.2502, -0.1231,  0.0766]],\n",
       "\n",
       "         [[ 0.1651,  0.1189,  0.7695,  ..., -0.3269, -0.0645,  0.2887],\n",
       "          [-0.3142,  0.1426, -0.3566,  ...,  0.1385,  0.2194, -0.5799],\n",
       "          [ 0.1760,  0.2181, -0.2877,  ...,  0.1804, -0.1474, -0.3382]],\n",
       "\n",
       "         [[ 0.2235,  0.1137, -0.4000,  ..., -0.6155,  1.0588,  0.2228],\n",
       "          [-1.0788, -0.4199,  0.7797,  ...,  0.6243, -0.0714, -0.5756],\n",
       "          [-0.6183,  0.3692,  0.0474,  ..., -0.1011,  0.4426, -0.0493]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.2753, -0.0681,  0.1028,  ...,  0.6002,  0.2900,  0.4822],\n",
       "          [ 0.5045, -0.0308,  0.5104,  ..., -0.0478, -0.2375, -0.0907],\n",
       "          [-0.4359, -0.2996, -0.1536,  ..., -0.5431, -0.8854, -0.4260]],\n",
       "\n",
       "         [[ 0.1586, -0.0488,  0.3176,  ..., -0.5844,  0.2627, -0.3929],\n",
       "          [ 0.3213, -0.3253, -0.1027,  ..., -0.5948,  0.5223, -0.2828],\n",
       "          [ 0.2955, -0.4100, -0.2750,  ..., -0.1606,  0.2762, -0.2582]],\n",
       "\n",
       "         [[ 0.0971,  0.2299, -0.1834,  ..., -0.4504, -0.0658, -0.2669],\n",
       "          [ 0.1400, -0.3831,  0.6720,  ..., -0.3681, -0.8031,  0.1748],\n",
       "          [-0.1700, -0.0456,  0.1336,  ..., -0.0482, -0.5464,  0.0869]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 2.3849e-03, -4.0944e-03, -3.1304e-02,  ...,  1.7468e-02,\n",
       "           -3.1626e-02,  6.3322e-03],\n",
       "          [ 4.4540e-01, -3.1040e-01, -1.1794e-01,  ...,  1.8083e-01,\n",
       "           -6.5698e-01,  1.8554e-01],\n",
       "          [ 3.1362e-01, -1.6949e-02,  8.2435e-02,  ...,  1.8642e-01,\n",
       "           -6.2152e-02, -1.3650e-03]],\n",
       "\n",
       "         [[ 2.4277e-02,  3.6330e-03, -6.0138e-03,  ...,  1.4288e-02,\n",
       "            1.0489e-01, -1.5343e-03],\n",
       "          [ 2.2391e-01,  1.9743e-02, -4.0961e-02,  ...,  1.2365e-01,\n",
       "            1.3665e+00, -4.9890e-01],\n",
       "          [ 5.6884e-02, -2.2191e-01,  1.2424e-01,  ..., -4.4080e-02,\n",
       "            1.1628e+00, -2.2450e-01]],\n",
       "\n",
       "         [[-9.0863e-03,  1.0754e-02, -1.4580e-02,  ...,  6.0248e-03,\n",
       "           -6.6425e-02,  2.0420e-03],\n",
       "          [ 3.0108e-01, -5.2735e-02,  7.4108e-02,  ...,  5.5825e-02,\n",
       "            9.2690e-02,  1.9669e-01],\n",
       "          [-3.4701e-02,  7.9122e-02, -8.1304e-03,  ..., -9.2779e-02,\n",
       "           -7.8263e-01, -3.0761e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.3881e-02, -1.4656e-02,  1.5755e-02,  ...,  1.6685e-02,\n",
       "           -2.1846e-02,  2.7511e-03],\n",
       "          [-1.0683e-01, -2.2474e-01,  2.9534e-01,  ...,  4.7153e-01,\n",
       "           -9.7876e-02, -1.0002e-01],\n",
       "          [-2.3477e-01, -2.2704e-01,  1.0594e-01,  ...,  5.6969e-01,\n",
       "           -4.2079e-01, -2.5534e-02]],\n",
       "\n",
       "         [[-9.3356e-04,  2.8466e-03, -1.3341e-03,  ..., -6.1186e-03,\n",
       "            4.1183e-02,  6.9150e-02],\n",
       "          [-1.7684e-01,  3.3298e-01,  2.8592e-01,  ..., -1.7629e-01,\n",
       "           -2.8164e-03,  2.6519e-01],\n",
       "          [-9.0190e-02,  1.4486e-01,  1.7442e-02,  ...,  2.3644e-02,\n",
       "           -1.4846e-02,  8.5615e-03]],\n",
       "\n",
       "         [[-5.5876e-03,  8.9970e-03, -1.0536e-02,  ...,  2.0604e-02,\n",
       "            1.9078e-03, -5.7176e-03],\n",
       "          [-1.9637e-01, -2.6206e-01, -6.4020e-02,  ...,  4.1436e-01,\n",
       "           -2.6590e-01, -3.4072e-03],\n",
       "          [-7.7609e-02, -1.7940e-01, -7.1738e-02,  ...,  2.1440e-01,\n",
       "            1.2491e-01,  4.9659e-02]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-0.8099,  0.6977, -0.2132,  ..., -0.3045,  0.1573, -0.4753],\n",
       "          [ 1.6454, -1.6932, -0.5028,  ...,  0.7381, -0.9223, -0.6607],\n",
       "          [ 1.7030, -3.3395, -0.0824,  ...,  1.7074, -5.3856,  0.1658]],\n",
       "\n",
       "         [[ 0.5359,  1.1804,  0.0887,  ..., -1.5554, -0.3476, -0.6216],\n",
       "          [ 0.0586, -0.6727, -0.5428,  ...,  0.2027,  2.6157,  0.5546],\n",
       "          [-1.1820, -0.1744, -1.3543,  ..., -0.8417, -1.2216,  1.9114]],\n",
       "\n",
       "         [[ 0.8199, -0.3886,  0.1374,  ..., -0.4417, -1.2663,  0.3678],\n",
       "          [-1.2357, -1.0197,  0.5082,  ...,  1.2615,  2.2998,  0.6705],\n",
       "          [ 0.1710, -1.0821, -0.6196,  ...,  0.1416, -0.0266, -0.0170]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8163, -1.5079,  0.3450,  ...,  0.2967, -0.6203,  0.4316],\n",
       "          [ 0.3203, -0.2323, -0.7128,  ..., -0.5898, -1.0626, -1.0516],\n",
       "          [-0.3372, -0.1976, -1.4555,  ..., -2.3306,  0.0353, -0.7601]],\n",
       "\n",
       "         [[-0.0811,  1.1075, -0.2104,  ...,  1.0041,  0.2734, -0.1314],\n",
       "          [-1.0315,  0.6330, -1.5727,  ..., -0.3775, -0.2737,  2.9471],\n",
       "          [-0.2572, -0.3509, -1.1346,  ...,  0.8018,  1.5349,  2.6817]],\n",
       "\n",
       "         [[-0.0750, -0.6442,  0.9529,  ..., -1.4230, -0.6896,  0.3863],\n",
       "          [ 1.2511, -0.1981, -1.0018,  ...,  0.8694,  0.1339,  0.0547],\n",
       "          [-1.1164, -0.8245,  1.9072,  ...,  1.1744, -0.2620, -0.3537]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[-3.3103e-02,  5.9567e-03,  1.5349e-03,  ..., -1.4430e-02,\n",
       "            2.1992e-02, -1.2515e-02],\n",
       "          [ 7.5969e-01,  2.3011e-01, -2.3792e-01,  ..., -1.8491e-01,\n",
       "           -4.7788e-02, -9.0361e-01],\n",
       "          [ 1.8898e-02, -5.2409e-02, -7.5797e-02,  ...,  8.5580e-02,\n",
       "           -4.5549e-01, -6.7927e-01]],\n",
       "\n",
       "         [[-9.4184e-03,  3.4105e-03,  1.9228e-03,  ...,  1.7345e-02,\n",
       "           -5.7222e-03, -2.9231e-03],\n",
       "          [ 3.4021e-01,  1.5521e-01,  1.0178e-01,  ...,  2.4316e-01,\n",
       "            5.1775e-02, -5.1115e-03],\n",
       "          [ 3.6630e-01, -2.1133e-01,  1.1669e-01,  ...,  9.9610e-03,\n",
       "            7.0821e-02,  1.6725e-01]],\n",
       "\n",
       "         [[-1.3363e-02,  1.0897e-02, -3.5096e-03,  ..., -1.0240e-02,\n",
       "           -1.2313e-02,  2.9822e-02],\n",
       "          [-8.0450e-01,  6.4399e-01, -4.6015e-01,  ..., -7.8794e-01,\n",
       "           -6.5996e-01,  1.6427e+00],\n",
       "          [-6.1667e-02, -5.5357e-01,  2.9372e-02,  ...,  1.1510e-01,\n",
       "           -1.3839e-01, -3.7333e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.5777e-03, -8.3079e-03, -1.0058e-02,  ..., -1.4193e-02,\n",
       "           -1.2077e-02, -1.0481e-02],\n",
       "          [-1.4606e-02,  1.0338e-01, -2.6052e-02,  ..., -3.2295e-01,\n",
       "           -1.7096e-02,  2.1858e-01],\n",
       "          [ 4.2493e-02,  1.8602e-01,  2.3245e-01,  ...,  3.5592e-01,\n",
       "           -7.4718e-02, -6.1660e-02]],\n",
       "\n",
       "         [[-1.4590e-03, -2.9989e-03, -4.6017e-03,  ..., -4.5542e-03,\n",
       "           -4.1941e-03,  1.1709e-03],\n",
       "          [-3.8056e-01, -3.4715e-01, -4.2011e-01,  ..., -1.7409e-01,\n",
       "           -6.2800e-01,  5.3896e-01],\n",
       "          [-5.2665e-02, -3.3772e-02, -4.1003e-03,  ...,  5.7674e-02,\n",
       "            2.6782e-01,  2.1023e-01]],\n",
       "\n",
       "         [[-4.4603e-03,  9.4471e-03,  3.6361e-02,  ...,  1.3832e-03,\n",
       "            3.2471e-03,  2.4879e-03],\n",
       "          [-7.5081e-01,  3.7902e-01,  3.9741e-01,  ...,  2.0280e-01,\n",
       "            2.2104e-01, -1.2699e-01],\n",
       "          [-4.2387e-01, -6.1069e-01,  1.7427e-01,  ..., -2.1782e-01,\n",
       "            8.8089e-01,  1.5270e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[ 0.3085, -0.2421,  0.1802,  ...,  0.1121, -0.2766,  0.3089],\n",
       "          [-1.2639,  0.0735,  0.1633,  ..., -0.0333, -0.5007,  0.1069],\n",
       "          [-0.3381,  0.1138,  0.1366,  ...,  0.0319, -0.1771, -0.0770]],\n",
       "\n",
       "         [[-0.3964,  0.3668,  0.5053,  ...,  0.0754, -0.3748, -0.9954],\n",
       "          [ 0.9422,  0.1369,  0.2085,  ..., -1.2303, -0.4612, -0.3647],\n",
       "          [ 0.2632, -0.1302, -0.1861,  ..., -1.1308,  0.2266, -0.1779]],\n",
       "\n",
       "         [[-0.0507, -0.0553,  0.8682,  ..., -0.0135,  0.0657, -0.1166],\n",
       "          [ 0.3010,  0.1576, -0.3000,  ..., -0.2414,  0.1347, -0.2870],\n",
       "          [ 0.0329, -0.0643, -0.0917,  ...,  0.0946, -0.2864,  0.2127]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.8219,  0.1407,  0.0068,  ..., -0.5777, -0.0532,  0.2702],\n",
       "          [-0.2704,  0.3841,  0.5293,  ..., -0.2976,  0.5809,  0.4705],\n",
       "          [ 0.0443, -0.5384,  0.2142,  ..., -0.1852,  0.5531,  0.2714]],\n",
       "\n",
       "         [[-0.0349, -0.2646, -0.6756,  ..., -0.3412, -0.0201,  0.4479],\n",
       "          [-0.3942, -0.2237, -0.0127,  ..., -0.6232, -0.3737,  0.1315],\n",
       "          [-0.0258,  0.1176, -0.7214,  ...,  0.0697, -0.1083,  0.4629]],\n",
       "\n",
       "         [[ 0.4827,  0.4367,  0.7654,  ...,  0.3741, -0.1999,  0.3515],\n",
       "          [ 0.4105, -0.3003, -0.6075,  ..., -0.4215, -0.1425, -0.2993],\n",
       "          [-0.2188, -0.4773,  0.3310,  ...,  0.5549, -0.0453,  0.0843]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 6.7659e-03, -1.6798e-02, -1.2931e-02,  ..., -1.6836e-02,\n",
       "           -1.4679e-03,  1.0788e-02],\n",
       "          [ 2.9727e-01, -1.7641e-01,  2.0282e-01,  ...,  3.8419e-01,\n",
       "           -2.0485e-01,  1.2214e-01],\n",
       "          [ 1.1732e-01, -1.9833e-01,  1.2983e-01,  ...,  8.7018e-02,\n",
       "           -1.2451e-01,  7.5004e-02]],\n",
       "\n",
       "         [[ 2.3948e-03,  2.3662e-03, -5.0065e-03,  ...,  6.4164e-03,\n",
       "            6.1962e-03,  9.2695e-03],\n",
       "          [-5.6330e-01,  1.0519e-01,  1.3788e-01,  ..., -4.8979e-01,\n",
       "           -5.0860e-01,  4.2529e-01],\n",
       "          [-1.6507e-01,  2.2026e-02,  8.1071e-02,  ..., -1.2419e-01,\n",
       "           -2.2290e-02,  3.4213e-02]],\n",
       "\n",
       "         [[-1.1942e-02,  4.3676e-03,  3.6905e-03,  ...,  2.3379e-02,\n",
       "           -3.6890e-03, -4.5083e-03],\n",
       "          [-9.6829e-02,  2.0427e-02,  8.0515e-02,  ...,  3.4431e-01,\n",
       "            5.2007e-02, -5.6872e-02],\n",
       "          [-2.6038e-01,  5.0281e-02, -2.1689e-01,  ...,  2.0949e-01,\n",
       "            9.6358e-04, -2.9823e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.5732e-03, -2.8012e-03,  3.0642e-03,  ...,  1.0237e-02,\n",
       "           -1.4467e-03, -1.4155e-03],\n",
       "          [-1.6544e-01, -3.0741e-01, -1.1749e-02,  ...,  2.8767e-02,\n",
       "            1.9065e-01, -2.9759e-01],\n",
       "          [ 1.6973e-01, -1.1871e-01,  8.2325e-02,  ...,  3.0865e-01,\n",
       "           -7.0432e-02, -2.3793e-01]],\n",
       "\n",
       "         [[-5.0598e-03, -1.4038e-02, -7.4111e-05,  ..., -4.9824e-02,\n",
       "            6.9581e-02, -1.0507e-02],\n",
       "          [-6.5295e-02, -9.3733e-03,  2.1566e-02,  ...,  5.3625e-02,\n",
       "            1.0344e-01,  5.3017e-02],\n",
       "          [-1.0002e-01, -1.9776e-01, -1.1975e-01,  ...,  1.9439e-01,\n",
       "           -1.7688e-01,  9.6332e-02]],\n",
       "\n",
       "         [[-6.7144e-03, -2.8538e-03, -4.7701e-03,  ..., -2.7432e-02,\n",
       "           -3.8199e-03, -1.2240e-03],\n",
       "          [-5.8664e-02,  2.0029e-02, -3.2568e-01,  ..., -2.1395e-01,\n",
       "            9.3797e-02,  9.9195e-02],\n",
       "          [ 1.7770e-02, -2.7419e-02, -1.2220e-01,  ..., -2.5619e-01,\n",
       "            3.6404e-02, -2.0735e-01]]]], grad_fn=<CloneBackward0>)), (tensor([[[[-1.0076e+00, -7.5340e-01,  5.3086e-01,  ...,  4.6840e-01,\n",
       "            1.0532e+00,  1.0503e+00],\n",
       "          [ 1.7502e-01,  7.7595e-01, -1.4426e+00,  ...,  2.2199e+00,\n",
       "            1.4974e+00, -2.5756e-01],\n",
       "          [-5.0115e-01,  4.5663e-01, -7.9532e-01,  ...,  2.1257e+00,\n",
       "           -1.0918e+00,  3.0022e+00]],\n",
       "\n",
       "         [[-1.1414e-01,  3.7568e-01,  7.1123e-02,  ...,  1.1775e+00,\n",
       "           -6.2628e-01,  1.6929e-01],\n",
       "          [ 5.6333e-01,  8.7235e-02,  9.1900e-01,  ..., -6.9791e-01,\n",
       "           -6.1484e-01,  8.3838e-01],\n",
       "          [ 2.1255e+00, -1.8299e+00,  2.1239e+00,  ..., -2.2377e+00,\n",
       "           -1.2283e+00,  1.9805e-01]],\n",
       "\n",
       "         [[ 8.5701e-01, -4.6428e-01,  6.7619e-01,  ..., -6.4295e-02,\n",
       "            1.1593e-01,  8.8639e-03],\n",
       "          [-2.8426e-01, -1.4550e+00, -7.0554e-01,  ..., -1.2822e+00,\n",
       "           -3.7066e-01,  1.1395e+00],\n",
       "          [-1.9940e+00,  7.7000e-01, -2.8597e+00,  ...,  2.7897e-01,\n",
       "            1.4000e+00,  4.6343e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 6.5425e-01,  8.2408e-01,  1.2620e-02,  ...,  4.2632e-01,\n",
       "           -3.0081e-01,  4.7751e-02],\n",
       "          [-8.9531e-01, -8.0023e-01,  1.9399e-01,  ..., -7.8588e-01,\n",
       "            7.9918e-02,  1.3983e+00],\n",
       "          [ 7.1560e-01,  1.8087e+00,  7.6084e-02,  ...,  2.8221e-01,\n",
       "            2.2887e+00,  1.5902e+00]],\n",
       "\n",
       "         [[-5.0564e-02, -1.7597e+00, -5.7445e-01,  ...,  6.1626e-01,\n",
       "            2.8255e-01,  2.9244e-01],\n",
       "          [-4.4518e-01,  1.8186e+00,  5.0497e-01,  ..., -4.8818e-01,\n",
       "            1.3412e+00,  8.7702e-01],\n",
       "          [-7.0579e-01,  2.8740e-01, -2.2352e-03,  ..., -1.3597e+00,\n",
       "            5.5492e-01, -9.9689e-02]],\n",
       "\n",
       "         [[ 6.0440e-01,  3.1558e-01,  6.3458e-01,  ...,  1.8526e-01,\n",
       "            4.7522e-02, -9.6277e-02],\n",
       "          [ 3.0975e-01, -1.2850e+00, -1.6226e+00,  ..., -6.3210e-01,\n",
       "           -6.1000e-01,  9.9480e-01],\n",
       "          [-1.8193e+00, -1.0324e+00, -4.9765e-01,  ..., -1.0414e+00,\n",
       "            6.9569e-01,  8.4227e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-2.6259e-03, -3.1840e-04, -2.4671e-03,  ..., -4.9073e-04,\n",
       "            5.2960e-04,  1.5640e-04],\n",
       "          [-9.1261e-02, -2.5988e-01, -3.4014e-01,  ..., -2.7624e-02,\n",
       "           -2.6632e-02, -1.5439e-01],\n",
       "          [-8.6391e-02, -5.3735e-01, -2.4412e-02,  ...,  6.5981e-02,\n",
       "           -4.1707e-01, -7.0022e-01]],\n",
       "\n",
       "         [[ 9.9399e-03,  3.3048e-03, -5.8104e-03,  ..., -1.0246e-04,\n",
       "           -4.7363e-04, -2.4317e-03],\n",
       "          [ 2.5622e-01,  5.2902e-01, -4.9545e-01,  ...,  2.2868e-02,\n",
       "            3.8069e-02,  2.0988e-01],\n",
       "          [ 3.7084e-01,  3.2088e-01, -6.5852e-01,  ..., -2.5705e-01,\n",
       "           -8.2258e-01,  2.8728e-01]],\n",
       "\n",
       "         [[ 9.6174e-03, -3.5428e-03,  1.8655e-03,  ...,  1.0404e-02,\n",
       "           -3.2546e-03, -1.2737e-03],\n",
       "          [ 1.8844e-01,  2.8746e-01,  3.8585e-01,  ...,  6.2992e-01,\n",
       "           -2.2696e-01,  2.1125e-01],\n",
       "          [-1.3114e-01,  1.7914e-01,  6.2851e-02,  ...,  3.1566e-01,\n",
       "           -3.4776e-01,  1.5583e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.2088e-03,  2.8000e-03, -5.5022e-03,  ..., -2.1205e-02,\n",
       "           -5.1618e-03, -4.2457e-03],\n",
       "          [ 2.3619e-01, -1.6854e-01, -1.5938e-01,  ..., -5.3698e-02,\n",
       "           -1.3572e-01, -3.7007e-01],\n",
       "          [ 7.0686e-01, -1.9758e-01, -3.7468e-01,  ..., -8.2795e-02,\n",
       "           -3.7093e-01, -3.9196e-01]],\n",
       "\n",
       "         [[ 3.1681e-03,  6.1639e-03,  8.1828e-04,  ..., -6.6903e-04,\n",
       "           -2.8707e-03,  3.7848e-03],\n",
       "          [-1.8790e-01,  1.9427e-01,  5.0726e-02,  ..., -3.5402e-02,\n",
       "            7.6290e-02,  6.4765e-02],\n",
       "          [ 9.2535e-03, -5.3848e-01,  2.2164e-01,  ...,  3.4821e-01,\n",
       "            7.3868e-02, -2.2729e-01]],\n",
       "\n",
       "         [[-8.5049e-04,  2.5104e-03,  3.5347e-03,  ...,  1.8697e-03,\n",
       "            1.6032e-03, -4.2679e-05],\n",
       "          [ 1.8134e-01,  1.2617e-01,  2.0702e-01,  ...,  7.7462e-02,\n",
       "            8.1425e-02,  3.9832e-02],\n",
       "          [-5.6440e-01, -5.5686e-01,  7.7366e-03,  ..., -1.4555e-01,\n",
       "           -1.6263e-01, -1.8732e-01]]]], grad_fn=<CloneBackward0>), tensor([[[[-0.2478, -0.3855, -0.1605,  ...,  0.7733, -0.2563, -0.3196],\n",
       "          [-0.7624,  0.7973,  1.0458,  ...,  0.4946, -0.6742,  0.4788],\n",
       "          [ 0.0313,  0.0928,  0.6385,  ..., -0.9157, -0.7217,  0.5032]],\n",
       "\n",
       "         [[-0.0537,  0.2064,  0.0617,  ..., -0.1449,  0.2948,  0.2288],\n",
       "          [ 0.4599,  0.0747, -1.7234,  ...,  0.0154,  0.0437,  0.5604],\n",
       "          [ 0.4104,  0.0185, -0.8379,  ...,  0.3276,  0.8694,  0.3868]],\n",
       "\n",
       "         [[ 1.1336,  0.2320,  0.4225,  ..., -0.6869,  0.5348,  0.6685],\n",
       "          [ 0.7665, -0.9769, -0.6508,  ..., -1.5750, -0.1748, -0.5513],\n",
       "          [ 1.0513, -0.2956,  0.4289,  ..., -0.8962,  0.4992,  0.1403]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0576,  0.5490,  0.2030,  ...,  0.3883,  0.4083,  0.0181],\n",
       "          [ 0.1857,  0.4452, -0.1152,  ..., -0.1795, -0.5823, -0.0355],\n",
       "          [-0.8401,  0.5059,  0.3360,  ...,  0.1005, -0.3140,  0.2442]],\n",
       "\n",
       "         [[-0.9707,  0.5837,  0.0053,  ..., -0.5551, -0.5135,  0.3053],\n",
       "          [-1.4307,  0.8490, -1.9002,  ..., -0.8223,  0.7934,  1.8711],\n",
       "          [-0.9800,  0.6420, -0.7818,  ...,  0.0423, -0.2258,  1.1204]],\n",
       "\n",
       "         [[ 0.2301,  0.0946,  0.0610,  ..., -0.4108,  0.1941,  1.0820],\n",
       "          [ 0.2003,  0.0667, -0.3193,  ...,  1.2633, -0.1002, -0.1311],\n",
       "          [-0.2041, -0.3159,  0.6594,  ...,  0.9155, -0.0397, -0.1774]]]],\n",
       "       grad_fn=<CloneBackward0>), tensor([[[[ 1.9369e-03,  4.2218e-04,  1.3064e-02,  ...,  1.2872e-03,\n",
       "           -4.2510e-03,  5.6916e-03],\n",
       "          [ 1.3722e-01,  3.2713e-01, -1.0484e-01,  ..., -1.1138e-01,\n",
       "            1.1014e-01,  1.1686e-01],\n",
       "          [-1.3118e-01,  2.4412e-01,  2.3183e-01,  ...,  1.2964e-01,\n",
       "           -1.9859e-02,  3.3195e-01]],\n",
       "\n",
       "         [[-5.4139e-03, -3.7529e-03, -1.6767e-03,  ..., -3.2886e-03,\n",
       "           -2.3118e-03, -4.6153e-04],\n",
       "          [-4.7091e-02,  3.3742e-01,  1.1711e-01,  ...,  4.4189e-02,\n",
       "           -2.7378e-01,  3.2531e-01],\n",
       "          [-6.0782e-03, -5.0074e-02, -4.1502e-02,  ...,  8.8565e-02,\n",
       "           -8.3225e-02,  1.1368e-01]],\n",
       "\n",
       "         [[ 3.6907e-03,  4.2214e-03,  3.0751e-03,  ..., -5.2486e-05,\n",
       "           -1.4493e-02,  5.7693e-04],\n",
       "          [-4.7605e-01,  1.4498e-01,  8.4578e-02,  ..., -4.3850e-02,\n",
       "            4.2552e-02, -1.0573e-01],\n",
       "          [-1.4773e-01,  1.2395e-01,  6.2835e-02,  ...,  1.1565e-01,\n",
       "           -1.0186e-01,  2.0991e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.4772e-02, -1.4229e-02,  3.8078e-04,  ...,  1.1455e-02,\n",
       "            3.6832e-04,  8.1450e-03],\n",
       "          [-2.8834e-01, -7.9261e-02,  4.6934e-01,  ..., -1.8034e-01,\n",
       "            4.7305e-01, -6.8379e-02],\n",
       "          [-1.4968e-01,  6.9038e-02,  7.5096e-02,  ..., -1.9076e-01,\n",
       "            2.8020e-01,  3.7629e-01]],\n",
       "\n",
       "         [[-2.5302e-03, -7.8202e-04, -2.8509e-04,  ...,  4.6571e-03,\n",
       "           -1.1963e-02,  6.8337e-03],\n",
       "          [-1.7911e-01,  3.8703e-01, -6.9719e-02,  ..., -8.7916e-02,\n",
       "           -2.8462e-01, -2.5950e-01],\n",
       "          [-1.0563e-01,  1.6859e-01,  1.1021e-01,  ..., -1.7011e-02,\n",
       "           -8.6497e-02, -9.6736e-02]],\n",
       "\n",
       "         [[ 7.5286e-03, -4.0444e-03,  1.9336e-02,  ..., -6.5930e-03,\n",
       "            1.3332e-02,  1.0115e-02],\n",
       "          [-4.8207e-02, -3.3513e-01,  1.8949e-01,  ...,  5.6590e-02,\n",
       "           -4.6037e-01, -5.9140e-01],\n",
       "          [-8.2830e-03, -1.0289e-01,  7.7964e-02,  ...,  4.4193e-02,\n",
       "           -2.0166e-01, -2.9000e-01]]]], grad_fn=<CloneBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-0.0343,  0.0126, -0.0042,  ...,  0.0129, -0.0028,  0.0008],\n",
       "         [ 0.0934, -0.0077,  0.5252,  ...,  0.1128, -0.2649,  0.3711],\n",
       "         [ 0.0818,  0.0801,  0.1946,  ...,  0.0695, -0.0471,  0.1733]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=torch.tensor([[1,2,3]]),error_points=torch.tensor([[0,1,0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
